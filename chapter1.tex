%TODO: коректні посилання
\chapter{Аналіз методів, алгоритмів і програмних засобів обробки планарних зображень}

\section{Аналіз задачі автоматизованого скринінгу}
\subsection{Задачі автоматизованого скринінгу в медицині}
Серед багатьох застосувань скринінгу в медицині, особливо нагальною є потреба в методах скринінгу для агресивних прихованих хвороб в регіонах, де в більшості населення немає доступу до професійного лікаря. Так, агресивними хворобами, для яких утруднене діагностування на ранніх стадіях є діабетична ретинопатія та рак шкіри. 

\subsubsection{Скринінг раку шкіри}
Рак шкіри є найбільш розповсюдженим видом злоякісних пухлин, і саме меланома є причиною більшості смертей від раку. Світова проблема захворюваності на меланому стрімко зростала за останні 50 років і стала проблемою, з якою намагаються боротися багато вчених з різних країн. 

Меланома є п'ятим за поширеністю раком серед чоловіків та шостим за поширеністю раком серед жінок [1]. Подібно до інших типів раку, ранні та легкі стадії візуально навряд чи можна розрізнити. В наш час, дерматологи оцінюють кожну родинку пацієнта, щоб виявити незвичні осередки або такі, що, швидше за все, є злоякісними. Якщо меланому помітити вчасно, її можна вилікувати незначними оперативними втручаннями.

\emph{Огляд літератури}
Недавні дослідження в галузі автоматичного виявлення злоякісних утворень пов'язані з найсучаснішими підходами до глибокого навчання в розпізнаванні зображень. Набагато менше робіт використовують класичне машинне навчання та сконструйовані дослідниками ознаки. 

Тут наведено найвпливовіші роботи в цій галузі. Так, Mustafa et al. [7] створив підхід з підібраними вручну ознаками (GrabCut для сегментації раку) та методом опорних векторів для класифікації ракових уражень. Також, Nasiri et al. [8] згенерував похідні зображення за допомогою декількох алгоритмів і використав на них метод k-найближчих моделей сусідів для вирішення завдання.

Брінкер та ін. [9] проводив експерименти із попередньо навченими на наборі даних ImageNet згортковими нейронними мережами, такими як ResNet-50, для класифікації ранніх стадій меланоми. Автори використали 4204 перевірених біопсією зображення меланоми та звичайних родинок. Крім того, були інтегровані новітні на той момент методи глибокого навчання: різні  темпи навчання для різних частин нейронної мережі, зменшення темпу навчання на основі функції косинуса, стохастичний градієнтний спуск з перезапуском для того щоб уникнути локальних мінімумів.

Коделла та ін. [10] запропонували систему сегментації та класифікації меланоми за дермоскопічними зображеннями шкіри. Для класифікації хвороб вони застосували ансамбль останніх методів машинного навчання, включаючи глибокі залишкові мережі, згорткові нейронні мережі тощо. Вони довели, що ансамблі здатні давати кращі результати, ніж моделі окремо.

Насірі та ін. [4] запропонували класифікацію уражень шкіри за допомогою глибокого навчання для раннього виявлення меланоми в системі міркувань на основі прецедентів (англ. case-based reasoning). Цей підхід був використаний для отримання схожих вхідних зображень із бази даних прецедентів запропонованої системи DePicT Melanoma Deep-CLASS для підвищення точності рекомендацій щодо запитуваної проблеми (наприклад, зображення родинки). Їх метод, що заснований на глибоких згорткових нейронних мережах, генерує ознаки з зображень, щоб використовувати їх у процесі пошуку в базі даних. Інтеграція цього підходу до DePicT Melanoma CLASS значно покращила ефективність класифікації зображень та якість рекомендаційної частини системи.

Дослідження в галузі багатозадачного навчання також проводили Сонг та ін. [5]. Вони запропонували нейронну мережу, яка може одночасно виконувати завдання детекції, класифікації та сегментації уражень шкіри, не вимагаючи додаткових етапів попередньої обробки або подальшої обробки. Подібну роботу представили Чен та співавт. [6], вони використали багатозадачну мережу U-Net для задачі детекції та сегментації.
Янг та ін. [11] запропонував більш складну багатозадачну модель, яка одночасно вирішує завдання сегментації уражень та дві незалежні задачі бінарної класифікації, використовуючи спільності та відмінності між завданнями.

\subsubsection{Скринінг діабетичної ретинопатії}
Діабетична ретинопатія є одним із найбільш загрозливих ускладнень діабету, при якому пошкодження сітківки викликає сліпоту. Вона пошкоджує кровоносні судини тканини сітківки, викликаючи витік рідини та погіршення зору. Поряд із захворюваннями, що призводять до сліпоти, такими як катаракта і глаукома, ретинопатія є одним із найпоширеніших захворювань, згідно зі статистикою США, Великобританії та Сінгапуру. 

Лікарі встановили чотири стадії діабетичної ретинопатії:
\begin{itemize}
	\item Легка непроліферативна ретинопатія, найраніша стадія, коли можуть виникати лише мікроаневризми;
	\item Помірна непроліферативна ретинопатія, стадію якої можна описати втратою здатності кровоносних судин до транспортування крові через  набряк з прогресуванням захворювання;
	\item Важка непроліферативна ретинопатія призводить до обмеженого кровопостачання сітківки через підвищений набряк великої кількості кровоносних судин;
	\item Проліферативна діабетична ретинопатія - це запущена стадія, коли фактори росту, що виділяються сітківкою, активізують проліферацію нових кровоносних судин, зростаючи вздовж оболонки сітківки в склоподібному тілі, заповнюючи око.
\end{itemize}

Щонайменше 56\% нових випадків можна уникнути за допомогою належного та своєчасного лікування та скринінгу очей. Однак, початкова стадія цього захворювання не має помітних для пацієнта ознак, і виявити його на ранній стадії є справжньою проблемою. Більш того, добре навчені діагности іноді не можуть вручну оцінити стадію за діагностичними зображеннями очного дна пацієнта. 

\emph{Огляд літератури}

Багато дослідницьких зусиль було присвячено проблемі раннього виявлення діабетичної ретинопатії. Перш за все, дослідники намагалися використовувати класичні методи комп'ютерного зору та машинного навчання, щоб забезпечити відповідне рішення цієї проблеми. 

Наприклад, Priya et al. запропонували підхід на основі комп'ютерного зору для виявлення діабетичної ретинопатії за допомогою кольорових зображень очного дна. Автори створили набір ознак із вихідного зображення, використовуючи класичні методи обробки зображень, і використали метод опорних векторів для бінарної класифікації. Їх метод досяг чутливості 98\%, специфічності 96\% та точності 97\% на тестовому наборі з 250 зображень. 

Крім того, інші дослідники намагалися використати інші моделі для багатокласової класифікації, наприклад, застосовуючи аналіз головних компонент до зображень та використовуючи дерева рішень, Баєсові класифікатори або метод найближчих сусідів {conde} з найкращими результатами 73.4\% точності та 68.4\% для F-міри, використовуючи набір даних із 151 зображення з різною роздільною здатністю.

Зі зростанням популярності підходів, заснованих на глибокому навчанні, з’явилися методи, які застосовують глибокі згорткові нейронні мережі до цієї проблеми. Пратт та ін. {pratt} розробили архітектуру нейронної мережі та використали аугментацію даних, яка може ідентифікувати складні ознаки захворювання, пов'язані із задачею класифікації, такі як мікроаневризми, ексудат та крововиливи в сітківку ока, і, отже, автоматично діагностувати стадію захворювання. Цей метод досяг чутливості 95\% і точності 75\% на 5000 валідаційних зображень. Крім того, є й інші роботи про використання глибоких штучних нейронних мереж від інших дослідників {lam, li}.

Asiri та ін. провели аналіз значної кількості доступних методів та наборів даних, висвітливши їх плюси та мінуси {asiri}. Крім того, автори вказали на проблеми, які слід вирішити при розробці та вивченні ефективних та надійних алгоритмів глибокого навчання для різних проблем діагностики діабетичної ретинопатії, та звернули увагу на напрямки подальших досліджень.

Інші дослідники також намагалися здійснити трансферне навчання за допомогою згорткових нейронних мереж. Хагош та ін. {hagos} спробував навчити InceptionNet V3 для класифікації 5 класів з нейронною мережею, натренованою на наборі даних ImageNet і досяг точності 90,9%. 
Сарки та ін. {sarki} провів дослідження з навчання різних архітектур, зокрема ResNet50, Xception, DenseNets та VGG за допомогою попереднього навчання на наборі даних ImageNet і досяг найкращої точності 81.3\%. Обидві групи дослідників використовували набори даних, які надавали APTOS та Kaggle.

%TODO: severstal
\subsection{Задача автоматизованого скринінгу на виробництві}
\subsection{Задачі автоматизованого скринінгу в метеорології}
Однією з найцінніших особливостей визначення кліматичної моделі Землі є поведінка хмар. Однак дослідження їх поведінки є однією з найскладніших частин, оскільки вимагає досконалого розуміння всіх процесів в атмосфері. Класифікація різних типів організації хмар допомагає покращити розуміння цих хмар, що, в свою чергу, допоможе нам побудувати кращі кліматичні моделі.

Дослідники з Інституту метеорології імені Макса Планка зібрали найбільший набір даних, що складається з приблизно 10 000 фотографій хмар в видимому спектрі з супутників Terra і Aqua MODIS. Завдяки краудсорсинговій спільноті Zooniverse, вони створили анотований набір даних, в якому хмари позначені чотирма типами масок: цукор, квітка, гравій, риба (англ. Sugar, Flower, Gravel, Fish). Однак, через недосконалу процедуру маркування фотографій, та через те, що маркування було зроблене непрофесіоналами, розмітка є частково помилковою. Так, маски сегментації класів містять багато пікселів, що належать до фону, та деякі хмари не мають масок сегментації.

\emph{Огляд літератури}
Щоб описати широкий спектр дослідницьких робіт, розглянуто роботи, що використовують як супутникові зображення в видимому спектрі, так і в інших діапазонах: мультиспектральні та інфрачервоні.

Один із методів виявлення хмар був розроблений Чжу та співавт. [2], вони запропонували метод під назвою Fmask (функція маски) для виявлення хмар та їхніх тіней на зображеннях із супутника Landsat 7. Fmask використовує підходи, засновані на правилах на основі фізичних властивостей хмар, щоб відокремити потенційні хмарні регіони від чистого неба. В якості вхідних даних метод використовує інформацію із семидіапазонних датчиків Enhanced Thematic Mapper (ETM) та Enhancet Themtic Mapper Plus (ETM+), якими обладнаний супутник Landsat. Альтернативний підхід був запропонований Harb et al. [3], автори проаналізували мультиспектральні дані середньої роздільної здатності з супутників програми CBERS. Даний алгоритм використовує набір математичних операцій над спектральними смугами, щоб покращити видимість хмар та їхніх тіней. 

Ці методи дають точні результати, але в значній мірі залежать від моделей датчиків (оскільки вони є методами, заснованими на правилах), а пропоновані рішення не є масштабованими до інших типів датчиків.

Hu et al. [4] представили більш загальне рішення, використовуюч методи комп'ютерного зору для виявлення декількох низькорівневих ознак, таких як колір, особливості текстури тощо. Для оцінки піксельних масок автори використали класичні алгоритми машинного навчання. 
Озкан та ін. застосували глибокі нейронні мережі (такі як Feature Pyramid Network) для сегментації хмар із низькоорбітальних RGB-зображень супутників Gokturk-2 та RASAT.


%TODO: Аналіз методів та алгоритмів обробки зображень
\section{Аналіз методів та алгоритмів обробки зображень}
\subsection{Особливості планарних зображень}
Глибина сцени незначна відносно відстані до камери
Масштаб об’єктів в сцені є постійним
\subsection{Feature detection}
\subsection{Задача класифікації зображень та відомі методи її розв’язання}
\subsection{Задача семантичної сегментації зображень та відомі методи її розв’язання}
Before 2000, we used several methods in digital image processing: threshold segmentation, region segmentation, edge segmentation, texture features, clustering and so on.
From 2000 to 2010, there are four main methods: graph theory, clustering, classification and combination of clustering and classification.


\section{Методи глибинного машинного навчання}
Глибинне машинне навчання - це частина більш широкого сімейства методів машинного навчання, заснованих на штучних нейронних мережах (ШНМ). Алгоритми глибинного машинного навчання моделюють високорівневі абстракції за допомогою глибинного графу з декількох шарів, що побудовані з лінійних, чи нелінійних перетворень.

\subsection{Загальні відомості про штучні нейронні мережі}
Штучні нейронні мережі — це обчислювальні системи, натхнені біологічними нейронними мережами. ШНМ ґрунтується на сукупності з'єднаних вузлів, що називають штучними нейронами (аналогічно до біологічних нейронів у головному мозку тварин). Кожне з'єднання (аналогічне синапсу) між штучними нейронами може передавати сигнал від одного до іншого. Штучний нейрон, що отримує сигнал, може обробляти його, й потім сигналізувати іншим нейронам, приєднаним до нього \cite{cs231n}.

\subsection{Згорткові нейронні мережі}
Революція в розпізнавання образів була зроблена за допомогою згорткових нейронних мереж (ЗНМ). Раніше, для задач розпізнавання використовувалися фільтри, які обиралися вручну, а після них використовувався класифікатор. Велика перевага нейронних мереж, полягає в тому, що потрібні лише тренувальні дані. На основі даних, фільтри і класифікатори навчаються автоматично. Це стало особливо потужним методом в завданнях розпізнавання зображень.
 
Дані з зображень захоплюється за допомогою операції згортки. Використовуючи згорткові ядра для сканування цілого зображення, потрібно вивчити порівняно небагато параметрів відносно повнозв'язних штучних нейронних мереж. Окрім безпосередньо операції згортки, сучасні ЗНМ використовують нелінійні функції активації, операції підвибірки та нормалізації. 

\paragraph{Згорткові шари}
Згорткові шари в глибоких нейронних мережах є основним блоком, що дозволяє формувати карти ознак.
\paragraph{Активації}
Функції активації забезпечують нелінійність нейронної мережі, виконуючи нелінійне перетворення карти ознак, отриманої від згорткового, або повнозв'язного шару. 

Найбільш популярними функціями активації в даний час є \cite{activations_survey}:
\begin{itemize}
	\item Сигмоподібна (Sigmoid) \cite{sigmoid}
	\item Гіперболічний тангенс (TanH) \cite{tanh}
	\item Лінійний ректифікатор (ReLU) \cite{relu}
	\item Нещільний лінійний ректифікатор (LeakyReLU) \cite{leakyrelu}
	\item SoftPlus \cite{softplus}
	\item Доповнена тотожна функція (Bent identity) \cite{bidentity}
	\item Swish \cite{swish}
	\item Mish \cite{mish}
\end{itemize}

Графіки зазначених функцій зображено на рис. \ref{fig:activations}.

\begin{figure}
	\centering
	\includegraphics[width=8cm]{activations.png}
	\caption{Графіки популярних функцій активації \cite{mish}}
	\label{fig:activations}
\end{figure} 

\paragraph{Підвибірки}
Шари підвибірки використовуються для зменшення просторової роздільної здатності та агрегації просторових ознак. В сучасних нейронних мережах використовуються шари підвибірки з функцією максимуму та середнього арифметичного. Також існують шари підвибірки з іншими функціями, але вони не набули загального застосування.

\paragraph{Нормалізації}
В сучасних нейронних мережах, шари нормалізації використовуються для стабілізації процесу навчання. Розподіл входів кожного шару змінюється під час навчання, оскільки змінюються параметри попередніх шарів. Це уповільнює навчання, вимагаючи нижчих темпів навчання та ретельної ініціалізації параметрів. 

Нормалізація проводиться над картами ознак, або їх частинами, приводячи їх до розподілу з середнім значенням 0 та стандартним відхиленням 1. Різні типи нормалізацій які застосовуються в нейронних мережах включають:

\begin{itemize}
	\item Пакетна нормалізація (batch normalization)\cite{batchnorm}
	\item Пошарова нормалізація (layer normalization)\cite{layernorm}
	\item Поекземплярна нормалізація  (instance normalization) \cite{inastancenorm}
	\item Групова нормалізація (group normalization) \cite{groupnorm}
\end{itemize}

Візуалізація різних типів нормалізації зображена на рис. \ref{fig:normalizations}. Тут $N$ - кількість карт ознак в пакеті,  $C$ - розмірність каналів карти ознак, $H$, $W$ - просторова розмірність одного каналу. Синім виділено набори ознак, на яких обчислюється середнє значення та стандартне відхилення. 

\begin{figure}
	\centering
	\includegraphics[width=16cm]{normalizations.png}
	\caption{Візуалізація різних типів нормалізації \cite{groupnorm}}
	\label{fig:normalizations}
\end{figure} 

\subsection{Визначні сучасні архітектури ШНМ класифікації зображень}

\paragraph{Нейронна мережа AlexNet} 

AlexNet є однією з найперших вдалих спроб застосування штучних згорткових нейронних мереж до зображень у великому масштабі. На відміну від більш ранніх робіт, автори досягли великого прориву на великому наборі даних ImageNet \cite{imagenet}.

Архітектура нейронної мережі є логічним продовженням розробок Я. Лекуна \cite{lenet} для розпізнавання рукописних літер за допомогою згорткових нейронних мереж. AlexNet складається з восьми шарів, перші п’ять з яких - згорткові, а останні три - повнозв’язними шарами. AlexNet використовує функцію активації ReLU, яка демонструвала покращені результати навчання в порівнянні сигмоподібною. Для збільшення поля зору, автори використали великий розмір ядра в перших згорткових шарах (11 та 5 пікселів відповідно)

Для того, щоб мати можливість навчання не великій кількості зображень порівняно високої роздільної здатності, автори розробили алгоритм паралельного навчання на декількох графічних процесорах (GPU).

\begin{figure}
	\centering
	\includegraphics[width=16cm]{alexnet.png}
	\caption{Граф нейронної мережі AlexNet \cite{alexnet}}
	\label{fig:alexnet_dag}
\end{figure} 

Також, автори вперше показали можливість використання карт ознак як міри подібності для вхідних зображень та необхідність регуляризації для запобігання перенавчанню. 

AlexNet вважається однією з найвпливовіших статей, опублікованих в галузі комп'ютерного зору, завдяки чому було опубліковано багато інших публікацій із використанням CNN та GPU для прискорення глибокого навчання. 


\paragraph{Нейронна мережа VGGNet}
VGGNet є інкрементальним покращенням архітектури AlexNet, в якому адресовано проблему продуктивності та систематизовано залежність точності від глибини нейронної мережі. Покращення продуктивності в порівнянні з AlexNet досягається заміною великих розмірів ядра згортки (11 і 5 у першому та другому згортковому шарі, відповідно) кількома згортками з розміром ядра 3х3 одна за одною. Також, за рахунок використання більшої кількості нелінійних активацій, підвищується дикримінаційна здатність нейронної мережі. Так, три згортки з ядром 3х3 одна за одною та кроком 1 мають той самий розмір поля зору що й одна згортка з розміром ядра 7х7, але кількість задіяних параметрів становить $3 \cdot (3^2 C^2)$ порівняно з $7^2 C^2$ параметрів для ядер розміром 7х7. Тут $C$ - це кількість вхідних та вихідних каналів згорткового шару. 

Автори порівнюють декілька нейронних мереж, які відрізняються лише кількістю шарів. Вихідна таблиця порівняння архітектур зазначена на рис. \ref{fig:vggnet_table}.

\begin{figure}
	\centering
	\includegraphics[width=16cm]{vgg.png}
	\caption{Порівняння архітектур VGGNet різної глибини \cite{vgg}}
	\label{fig:vggnet_table}
\end{figure} 

В даній роботі було продемонстровано, що глибина нейронної мережі вигідна для точності класифікації, а також того, що можна досягти високої точності за допомогою звичайної архітектури згорткової нейронної мережі. 

\paragraph{Сімейство архітектур нейронних мереж Inception}
Мережа Inception \cite{inception}стала важливою віхою в розвитку згорткових нейронних мереж. До цієї роботи, згорткові нейронні мережі являли собою послідовні операції згортки, активації та підвибірки. До сімейства Inception входять нейронні мережі, що були спеціально розроблені для підвищення точності та продуктивності. 

Автори розглядають проблему варіативності розміру об'єктів на зображеннях: вибір правильного розміру ядра для операції згортки стає важким. Більше ядро краще підходить для виділення інформації, яка розповсюджена більш глобально, а менше ядро - для інформації, яка розповсюджується локально.

Для вирішення цієї проблеми, автори використовують декілька операцій згортки з різними розмірами ядра в одному шарі нейронної мережі. Оскільки виконання згорток з великою кількістю каналів є довгою операцією, автори зменшують кількість каналів за допомогою згорток з розміром ядра 1х1. Один такий шар називається \textit{блоком Inception}, архітектура якого  зображена на рис. \ref{fig:naive_inception}

\begin{figure}
	\centering
	\includegraphics[width=8cm]{inception_v1_1.png}		\includegraphics[width=8cm]{inception_v1_2.png}
	\caption{Блок-схема Inception v1 до та після розкладу\cite{inception}}
	\label{fig:naive_inception}
\end{figure} 

Нейронна мережа GoogLeNet \cite{inception_v1} складалася з 22 шарів в глибину та була найглибшою на той час. Для того, щоб запобігти затуханню градієнтів, використовувалися додаткові класифікатори на менш глибоких рівнях, результати яких було додано до функції втрат.

В наступних роботах \cite{inception_v2}, автори продовжують ідею зменшення кількості операцій, для чого вводять розкладання згорток з великим розміром ядра на декілька менших згорток. Так, автори пропонують розкладання згортки з ядром розміру 5х5 на дві згортки з розміром ядра 3х3, а згортки з розміром ядра 3х3 на дві згортки розміру 1х3 та 3х1 відповідно. Це дозволяє зменшити кількість параметрів в нейронній мережі при збереженні глибини, зменшуючи необхідні ресурси для обчислення.

Схема блоків Inception v2 та v3 зображено на рис. \ref{fig:inception_v2}
\begin{figure}
	\centering
	\includegraphics[width=8cm]{inception_v2_1.png}		\includegraphics[width=8cm]{inception_v2_2.png}
	\caption{Блок-схема Inception v2 до та після розкладу\cite{inception_v2}}
	\label{fig:inception_v2}
\end{figure} 

В нейронній мережі Inception v2 вперше було застосовано метод пакетної нормалізації \cite{batchnorm}, що дозволило значно пришвидшити навчання. Також, було показано, що додаткові класифікатори не мають впливу на затухання градієнтів, але мають властивості регуляризації, особливо на останніх стадіях навчання.

\paragraph{Сімейство архітектур нейронних мереж ResNet}

Сімейство нейронних мереж ResNet було засновано на новому методі залишкового навчання, що був представлений в тій самій роботі, що й архітектура нейронної мережі \cite{resnet}. Автори розв'язують проблему того, що збільшення глибини нейронної мережі зменшує точність як на навчальному, так і на тестувальному наборі даних. 

В роботі показано, що метод стохастичного градієнтного спуску не придатний до оптимізації глибоких нейронних мереж. Так, наведено експеримент, в якому між шарами неглибокої нейронної мережі використано шари тотожного відображення або додаткові згорткові шари з функцією активації. Показано, що для більш глибокої нейронної мережі має існувати рішення з точністю не меншою, ніж для неглибокої нейронної мережі, але, на практиці, точність більш глибокої нейронної мережі залишається нижчою. Для збільшення глибини нейронних мереж без втрати точності, в роботі запропоновано метод залишкового навчання. 

Нехай $\mathcal{H}_l(x_{l-1})$ - це відображення, що задається декількома шарами нейронної мережі, де $x_{l-1}$ - це вхідний тензор до першого з цих шарів. Якщо припустити, що декілька нелінійних шарів можуть апроксимувати будь-яку нелінійну функцію, то логічно припустити, що вони можуть апроксимувати і залишкову функцію: $\mathcal{F}_l(x_{l-1})=\mathcal{H}_l(x_{l-1}) - x_{l-1}$. Тоді, оригінальне відображення стає $\mathcal{H}_l(x_{l-1})=\mathcal{F}_l(x_{l-1}) + x_{l-1}$. Хоча ці відображення є еквівалентними, алгоритму оптимізації <<простіше>> таким чином вивчити тотожну функцію, оскільки для цього достатньо наблизити ваги шарів нейронної мережі до нуля.

В роботі автори пропонують блоки нейронних мереж, що складаються з декількох згорткових шарів з функцією активації ReLU та об'єднання з вхідним тензором першого шару. Схема таких блоків зображена на рис. \ref{fig:res_block}

\begin{figure}
	\centering
	\includegraphics[width=10cm]{res_blocks.png}
	\caption{Схема залишкових блоків нейронних мереж ResNet18 та ResNet50 \cite{resnet}}
	\label{fig:res_block}
\end{figure} 

Завдяки використанню таких блоків, стала можливою побудова надзвичайно глибоких нейронних мереж. В експериментах, автори запропонували архітектури, що складаються з 18, 34, 50, 101, 152 та 1202 згорткових шарів, та можуть бути оптимізовані стандартними методами (наприклад, методом стохастичного градієнтного спуску).



\paragraph{Сімейство архітектур нейронних мереж DenseNet}

Архітектура нейронних мереж сімейства DenseNet \cite{densenet} є логічним продовженням ідей, запропонованих в архітектурі ResNet. В даній архітектурі також використовується метод залишкового навчання, але, на відміну від архітектури ResNet, послідовні шари поєднані кожний з кожним. 

Для того, щоб підвищити об'єм інформації, яким обмінюються шари нейронної мережі, автори пропонують ввести пряме з'єднання від кожного шару до всіх наступних:
\begin{equation}
	x_l = \mathcal{H}([x_0, x_1, x_3 ... x_{l-1}])
\end{equation}
де $[x_0, x_1, x_3 ... x_{l-1}]$ - конкатенація всіх карт активації з шарів від $0$ до $l-1$. 

Така зміна архітектури дозволила останнім шарам перевикористовувати ознаки з попередніх, що, в свою чергу, дозволило зменишити кількість параметрів в нейронній мережі та підвищити точність.

Граф нейронної мережі DenseNet зображено на рис. \ref{fig:densenet}.

\begin{figure}
	\centering
	\includegraphics[width=16cm]{densenet.png}
	\caption{Архітектура нейронної мережі DenseNet \cite{densenet}}
	\label{fig:densenet}
\end{figure} 

На практиці, хоча архітектура дозволяє досягти більшої точності з використанням меншої кількості параметрів, на час виходу роботи, більшість реалізацій, заснованих на популярних фреймворках машинного навчання вимагали більшої кількості оперативної пам'яті через необхідність копіювання карт активацій для кожного з шарів. Таким чином, наївна реалізація архітектури DenseNet потребувала $\mathcal{O}(\frac{L(L-1)}{2})$ оперативної пам'яті. З розвитком фреймворків машинного навчання, була реалізована можливість перевикористання тензорів без копіювання, що дозволило знизити потребу оперативної пам'яті до $\mathcal{O}(L)$ \cite{efficient_densenet}.


\paragraph{Сімейство архітектур нейронних мереж ResNeXt}
Архітектури сімейства ResNeXt \cite{resnext} є логічним продовженням методів, застосованих в архітектурах Inception та ResNet. Основною ідеєю цієї архітектури є використання набору з декількох простих однакових трансформацій в просторі низької розмірності в кожному з шарів нейронної мережі. 

Так, автори пропонують розділити згортковий шар на декілька паралельних трансформацій, результати з яких агрегуються:

\begin{equation}
	\mathcal{H}(x_{l-1}) = \sum_{i=1}^{C} \mathcal{T}_i(x_{l-1})
\end{equation}

де $\mathcal{T}_i$ - це будь-яка нелінійна функція, що проектує вхід $x_{l-1}$ до простору низької розмірності, трансформує його, та проектує результат назад в простір високої розмірності. Тут $C$ - потужність множини трансформацій. 

Графічну репрезентацію блоку ResNeXt зображено на рис. \ref{fig:resnext}

\begin{figure}
	\centering
	\includegraphics[width=12cm]{resnext.png}
	\caption{Графічна репрезентація блоку ResNeXt \cite{resnext}}
	\label{fig:resnext}
\end{figure} 

В роботі автори проводять експерименти відносно важливості параметру $C$ відносно інших, та показують, що збільшення $C$ підвищує точність більше, ніж підвищення кількості каналів згорток. Також, в роботі показано, що такий метод здатний створювати кращі репрезентаціїї вхідних даних.

\paragraph{Сімейство нейронних мереж EfficientNet}
Сімейство нейронних мереж EfficientNet також є логічним продовженням архітектури ResNet. В даній роботі вперше розглядається узагальнений метод масштабування згорткових нейронних мереж для збільшення точності під заданий обчислювальний бюджет. 

Автори пропонують метод, за допомогою якого можна масштабувати одночасно вхідну роздільну здатність зображення, глибину та ширину нейронної мережі. Нведено спостереження, що ці величини не є незалежними.

Інтуїтивно зрозуміло, що для зображень із більш високою роздільною здатністю необхідно збільшити глибину мережі, щоб більші збільшити поле зору та захопити більше пікселів у більшому зображенні. Відповідно, також необхідно збільшувати ширину мережі, коли роздільна здатність збільшується, для того, щоб мати можливість розпізнати більшу кількість ознак на зображеннях із високою роздільною здатністю. 

Для цього, структура нейронної мережі формулюється як задача максимізації точності в залежності від ширини, глибини та роздільної здатності:

\begin{align}
& max \; Accuracy(\mathcal{N}(d, w, r)):
\\
& \mathcal{N}(d, w, r) = \bigodot_{i=1..s} \hat{\mathcal{F}}_i^{d \cdot \hat{L_i}}(X_{(r \cdot \hat{H_i}, r \cdot \hat{W_i}, w \cdot \hat{C_i)}})
\end{align}

де $w, d, r$ - параметри масштабування ширини, глибини та роздільної здатності відповідно,

та $\hat{\mathcal{F}}_i$ - функція, що задається архітектурою шару,

$\hat{L}_i$, $\hat{H}_i,\hat{W}_i$ - розміри карти активації, а $\hat{C}_i$ - кількість каналів. 

Також, пропонується визначення залежності між $d, w,  r$: 

\begin{align}
& d = \alpha^\phi
\\
& w = \beta^\phi
\\
& r = \gamma^\phi
\end{align}

так, що виконується наступне:

\begin{align}
&\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2
\\
&\alpha \geq 1, \beta \geq 1, \gamma \geq 1
\end{align}

де $\phi$ - множник, що задається в залежності від наявного обчислювального бюджету.

Завдяки цьому методу масштабування, автори представляють 8 варіантів нейронної мережі, що відрізняється необхідною точністю та продуктивністю: від найменшої до найбільшої. 

\subsection{Визначні архітектури ШНМ для сегментації зображень}
\paragraph{Повністю згорткова нейронна мережа}(\textit{англ. Fully-convolutional network, FCN}) \cite{fcn} - це перша нейронна мережа, що використовувала лише шари згортки, активації та підвибірки, не використовуючи повнозв'язних шарів в задачі семантичної сегментації.

Основною перевагою цього підходу стала можливість навчання та прогнозування на зображеннях різного розміру, без зміни параметрів нейронної мережі. 

В даній роботі вперше було застосовано індуктивний переніс знань з задачі класифікації на задачу сегментації. Для цього перші шари нейронної мережі було інінціалізовано за допомогою параметрів, отриманих після тренування на наборі даних ImageNet \cite{imagenet}. 

Для отримання масок сегментації, повнозв'язні шари було сконвертовано в згорткові, а також було додано інформацію з перших шарів для збереження просторових ознак. Граф нейронної мережі FCN зображено на рис. \ref{fig:fcn_dag}.

\begin{figure}
	\centering
	\includegraphics[width=16cm]{fcn.png}
	\caption{Граф нейронної мережі FCN \cite{fcn}}
	\label{fig:fcn_dag}
\end{figure} 

Тут, \textit{pool[1-5]} - карти активації після відповідних операцій підвибірки, а \textit{upsampled} - карти активації після білінійної інтерполяції. 


\paragraph{Нейронна мережа SegNet}

SegNet (\textit{сегментаційна мережа}) \cite{segnet}- перша повністю згорткова нейронна мережа, що використовувала архітектуру типу "енкодер-деокдер" для вирішення задачі семантичної сегментації. В даній архітектурі, енкодер стискає семантичне представлення до більш компактного простору. Роль декодера полягає у відображенні з компактного простору з низькою роздільною здатністю у карту актавації повного розміру для класифікації кожного пікселя.

Архітектура мережі енкодера топологічно ідентична 13 згортковим шарам мережі VGG16 \cite{vgg}.

Новина SegNet полягає в тому, як декодер підвищує розширення карт активаціїї низького розширення. Так, декодер використовує індекси операцій підвибірки, обчислені на відповідному шарі енкодера, для нелінійного підвищення розширення. Карти активації підвищенного розлирення виявляються розрідженими, для того, щоб зробити їх щільними, виконується декілька операцій згортки. Структура SegNet зображена на рисунку \ref{segnet_dag}.

\begin{figure}
	\centering
	\includegraphics[width=16cm]{segnet.png}
	\caption{Граф нейронної мережі SegNet \cite{segnet}}
	\label{fig:segnet_dag}
\end{figure} 

\paragraph{Нейронна мережа PSPNet}
Pyramid Scene Parsing Network (PSPNet) - це подальший розвиток ідеї FCN, в якому виправлено декілька значних проблем архітектури FCN. 
Серед основних проблем FCN автори відзначили:

\begin{itemize}
	\item Відсутність можливості обробки інформації про контекст рівня зображення
	\item Різний розмір та вигляд різних об'єктів одного класу
\end{itemize}

Для того, щоб вирішити ці проблеми, автори запропонували додати до архітектури FCN модуль пірамідальної підвибірки (\textit{англ. Pyramid Pooling Module }). Цей модуль складається з чотирьох операцій підвибірки з різними масштабом, що дозволяє захопити контекст зображення на різних рівнях. 

Структура PSPNet та модуля підвибірки зображена на рисунку \ref{pspnet_dag}.


\begin{figure}
	\centering
	\includegraphics[width=16cm]{pspnet.png}
	\caption{Граф нейронної мережі PSPNet \cite{pspnet}}
	\label{fig:pspnet_dag}
\end{figure} 

\paragraph{UNet}

U-Net (\textit{U-подібна мережа}) \cite{unet} - повністю згорткова нейронна мережа, що складається з симетричних енкодера та декодера, в яких, на відміну від SegNet замість індексів підвибірки, до декодера передаються карти активації енкодера напряму, через операцію конкатенації. Окрім спрощення кодування, така організація мережі дозволила спростити навчання, оскільки передча інформації стала повністю диференційованою. 

Також, для забезпечення можливості навчання на малій кількості даних, використовується велика кількість різноманіних аугментацій вхідних зображень, щоб синтетично збільшити розмір набору даних. 

Для більш чіткого розподілення близько розташованих об'єктів, автори запропонували спеціальну зважену функцію втрат, що забезпечувала більш чітке формування границь об'єктів. 

Хоча, оригінально ця архітектура була запропонована для семантичної сегментації планарних мікроскопічних знімків, подальші дослідження показали її застосовність в широкому спектрі задач семантичної сегментації. 

Структура нейронної мережі U-Net зображена на рисунку \ref{fig:unet_dag}.

\begin{figure}[H]
	\centering
	\includegraphics[width=14cm]{unet.png}
	\caption{Граф нейронної мережі U-Net \cite{unet}}
	\label{fig:unet_dag}
\end{figure} 

\paragraph{LinkNet}
Архітектура нейронної мережі LinkNet є подальшим розвитком архітектури U-Net, і також складається з енкодера та декодера, що поєднані через проміжні карти активації. 

На відміну від U-Net, LinkNet використовує ResNet в якості енкодера, а також для поєднання карт активації енкодера та декодера використовує операцію суми, замість конкатенації. Це дозволило значно зменшити кількість параметрів, підвищити швидкість тренування при збереженні точності сегментації. 

Також, автори LinkNet вперше використали індуктивний переніс з ImageNet виключно до енкодера, залишивши декодер випадково ініціалізованим на момент початку навчання. 

Структура нейронної мережі LinkNet зображена на рисунку \ref{fig:linknet_dag}.

\begin{figure}
	\centering
	\includegraphics[width=10cm]{linknet.png}
	\caption{Граф нейронної мережі LinkNet \cite{linknetnet}}
	\label{fig:linknet_dag}
\end{figure} 

В подальших роботах, автори зазвичай називають свої архітектури U-Net-подібними, хоча, насправді, вони є ближчими до LinkNet. \cite{smth, smth2, smth3}

\paragraph{Feature Pyramid Network}
Вперше розроблена для задач детекції об'єктів, Feature Pyramid Network (FPN) \cite{fpn_original}, була успішно застосована для задач семантичної сегментації \cite{fpn_segmentation}.

Структура FPN складається з двох шляхів: на одному з них зменшується просторова та розмірність збільшується глибина карт ознак (шлях знизу вверх), на іншому - навпаки (шлях зверху-вниз). Шлях знизу-вверх представляє собою звичайну згорткову нейронну мережу (зазвичай, з сімейства ResNet). Шлях зверху-вниз представляє собою послідовність згорток та операцій підвищення розширення. 

Для отримання карти активації, з якої буде отримано результат семантичної сегментації, всі рівні декодера оброблюються двома операціями згортки, після чого приводяться до однакового просторового розширення. З цієї карти ознак отримується фінальна карта сегментаціїї за допомогою операції згортки.

Структура нейронної мережі FPN для семантичної сегментації зображена на рисунку \ref{fig:fpn_dag}.

\begin{figure}
	\centering
	\includegraphics[width=17cm]{fpn_seg.png}
	\caption{Граф нейронної мережі FPN \cite{fpn_segmentation}}
	\label{fig:fpn_dag}
\end{figure} 

\paragraph{Сімейство нейронних мереж DeepLab}
Нейронні мережі з сімейства DeepLab \cite{deeplabv2, deeplabv3} побудовані навколо одного алгоритму підвибірки з розширеної просторової піраміди (\textit{англ. Atrous Spatial Pyramid Pooling (ASPP)} та архітектури eнкодер-декодер. ASPP - це модуль для передискретизації карт активації в декількох масштабах до виконання операції згортки. Це дозволяє за одну операцію згортки виділити ознаки в різних масштабах, а також корисний контекст зображення. 

Замість того, щоб насправді передискретизовувати карти ознак як в PSPNet, зміна масштабу виконується за допомогою фільтрів згортки різного розширення.

Структура блоку ASPP зображена на рисунку \ref{fig:aspp_block}.

\begin{figure}
	\centering
	\includegraphics[width=14cm]{aspp_block.png}
	\caption{Структура блоку ASPP в нейронних мережах DeepLab \cite{deeplabv2}}
	\label{fig:aspp_block}
\end{figure} 

\paragraph{Object-contextual Representations Networks}

\section{Трансферне навчання}
Трансферне навчання – це проблема машинного навчання, яка фокусується на збереженні знань, отриманих під час вирішення однієї задачі з подальшим застосуванням її результатів до іншої, але близької до неї. З практичної точки зору, повторне використання або передача інформації з раніше засвоєних завдань для вивчення нових може значно підвищити ефективність їх розв’язання [savchenko]. Деякі дослідники [] вважають багатозадачне машинне навчання підмножиною трансферного навчання.
  
На відміну від трансферного навчання, в багатозадачному навчанні немає різниці між задачами, та завдання стоїть підвищити продуктивність на всіх задачах одночасно.

\section{Методи багатозадачного глибинного машинного навчання}
Багатозадачне машинне навчання - це один з підходів машинного навчання, в якому одночасно вирішуються кілька навчальних задач, використовуючи спільності та відмінності між ними. Кожна з таких задач може бути загальною навчальною задачею, такою як навчання з учителем (наприклад, задача класифікації, або регресії), навчання без вчителя (кластеризація), напівавтоматичне навчання, завдання навчання з підкріпленням, або моделі на графах. Передбачається, що всі ці навчальні задачі або хоча б частина з них пов'язані одна з одною. 

В цьому випадку виявлено \cite{caruana}, що спільне навчання на цих задачах може призвести до значного підвищення продуктивності в порівнянні з навчанням на кожній задачі окремо. Отже, багатозадачне навчання направлено на підвищення якості узагальнення кількох пов'язаних задач. Далі будуть розглядатися виключно задачі контрольованого навчання. 

Подібно до людського навчання, корисно вивчати кілька навчальних задач разом, оскільки знання, що містяться в одній задачі, можуть бути використані іншими. Наприклад, людина, що вивчає математику та математичну статистику, може використати цей досвід для вивчення інших схожих галузей, наприклад, машинного навчання.

\subsection{Визначення багатозадачного машинного навчання}
Для n навчальних задач ${T_1, T_2, … T_n}$, де всі задачі або їх підмножина пов’язані, багатозадачне навчання (БЗМН) має на меті допомогти покращити вивчення моделі для $T_i$, використовуючи знання, що містяться в усіх n або деяких з них задчах.

В літературі визначається декілька видів БЗМН:

\emph{Однорідне БЗМН}: Для кожної з задач прогнозується лише один вихід. Наприклад, розпізнавання цифр MNIST зазвичай використовується для оцінки алгоритмів БЗМН. В цьому разі, воно розглядається як 10 завдань бінарної класифікації.

\emph{Неоднорідне БЗМН}: Для кожної з задач прогнозується свій набір виходів. Наприклад, сучасні задачі детектування об'єктів потребують одночасного прогнозування класу об'єкта (задача класифікації), його положення на зображенні та розмірів (задача регресії).
Для задач контрольованого навчання, кожна задача $T_i$ містить в собі набор даних $D_i$, що складається з $m$ елементів…

Дослідники вважають, що коли різні задачі використовують однакові набори вхідних даних, БЗМН зводиться до задачі класифікації або регресії з кількома виходами. Однак, новіші дослідження визначають БЗМН на одному наборі вхідних даних, або незалежно від кількості наборів вхідних даних.

В методах БЗМН, заснованих на глибоких нейронних мережах, виділяють дві групи відповідно до способу розподілу параметрів між різними задачами. 

\subsection{Жорсткий розподіл параметрів}
Зазвичай, архітектури згорткових нейронних мереж для багатозадачного навчання складаються зі спільного енкодера, який виділяє ознаки на вхідному зображенні (shared features), та окремих згорткових і/або повнозв’язних шарів для кожної з задач. 

В найпершій роботі \cite{deepfacealign}, що використовує цей принцип, було запропоновано архітектуру нейронної мережі Task-Constrained Deep Convolutional Network (TCDCN) для детекції ключових точок лиця використати додаткові задачі регресії пози голови та класифікації атрибутів. Архітектуру TCDCN зображено на рис. \ref{fig:deepfacealign}. 

\begin{figure}
	\centering
	\includegraphics[width=16cm]{deepfacealign.png}
	\caption{Архітектура нейронної мережі TCDCN \cite{deepfacealign}}
	\label{fig:deepfacealign}
\end{figure} 

Модифікацією попереднього підходу є багатозадачні каскадні мережі \ref{mcdcn}, в яких результати одних задач використовуються як доповнення до вхідної інформації для наступних. Така організація шарів дозволяє підвищити точність на більш складних завданнях при правильному підборі черги задач за складністю. Загальну архітектуру зображено на рис. \ref{fig:mcn}. Схожий підхід також використовується в деяких архітектурах нейронних мереж для розв'язання задачі детекції об'єктів \cite{rcnn, faster_rcnn, mask_rcnn}

\begin{figure}
	\centering
	\includegraphics[width=8cm]{mcn.png}
	\caption{Архітектура багатозадачної каскадної мережі \cite{mcn}}
	\label{fig:mcn}
\end{figure} 

Також, можлива інтеграція модулів уваги як до енкодера, так і до специфічних до задачі шарів. Таким чином, обчислення ознак відбувається не лише за рахунок параметрів енкодера, а і через специфічні до задачі модулі, що розташовані всередині мережі. Це дозволяє покращити репрезентації ознак для конкретних задач.


\subsection{М’який розподіл параметрів}
В архітектурах, що використовують м'який розподіл параметрів використовується декілька енкодерів для кожної з задач, між шарами яких відбувається обмін інформацією.

Наприклад, в роботі Cross-stitch networks (перехресні мережі)\cite{crossstitch} представлена архітектура багатозадачної нейронної мережі, в якій кожній задачі відповідає окремий енкодер, але входом кожного шару кожного з енкодерів є лінійна комбінація виходів попереднього шару всіх енкодерів. Коефіцієнти лінійної комбінації також вивчаються за допомогою градієнтного спуску разом з іншими параметрами нейронної мережі. На рисунку \ref{fig:cross_stitch_block} зображено архітектуру блоку лінійної комбінації для шарів двох нейронних мереж.

\begin{figure}
	\centering
	\includegraphics[width=8cm]{cross_stitch_block.png}
	\caption{Архітектура cross-stitch блоку \cite{crossstitch}}
	\label{fig:cross_stitch_block}
\end{figure} 

Узагальненням ідеї переносу інформації між послідовними шарами декількох специфічних до задачі нейронних мереж є шлюзові мережі (\emph{англ. Sluice networks}). Вхід до кожного з шарів також є лінійної комбінацією виходів попередніх шарів для кожної з задач. На відміну від перехресних мереж, кожний шар нейронної мережі розділений на окремі підпростори: для конкретної задачі та спільний між всіма задачами. Також, додається обмеження на ортогональність підпросторів через додавання функції втрат, що мінімізує квадратовану норму Фробеніуса (матричну $L^2$ норму) між підпросторами в кожному з шарів.

Подальший розвиток шлюзових мереж було запропоновано в роботі <<Нейромережеве дискримінтивне зменшення розмірності>>(\emph{англ. Neural Discriminative Dimensionality Reduction, NDDR}) \cite{nddr}. Тут, замість використання лінійної комбінації шарів між різними задачами, виконується згортка конкатенованих виходів шарів всіх задач з ядром розміру 1х1 з виконанням подальшої нормалізації. Архітектуру NDDR зображено на рисунку \ref{fig:nddr}. 

\begin{figure}
	\centering
	\includegraphics[width=12cm]{nddr.png}
	\caption{Архітектура згорткової нейронної мережі NDDR \cite{nddr}}
	\label{fig:nddr}
\end{figure} 

Суттєвим недоліком методів, що використовують м'який розподіл параметрів є постійне збільшення кількості параметрів нейронної мережі, що, в свою чергу, збільшує необхідну кількість ресурсів для роботи такої нейронної мережі зі збереженням продуктивності. 

\subsection{Дистиляція прогнозів моделі}
Дистиляція прогнозів замість вивчення ознак, спільних до декількох задач, базується на використанні результатів для одних задач, як засобу вивчення ознак для інших. Наприклад, при одночасному вивченні задач класифікації та сегментації, карти сегментації можуть бути використані для подальшої класифікації зображення.

Першою роботою, що використала цей принцип була PAD-Net (Prediction-and-Distillation Network) \cite{padnet}. В ній використовувалися результати задач семантичної сегментації, регресії карти глибини, регресії карти поверхневих нормалей та детекції контурів для отримання уточнених результатів в задачах регресії карти глибини та семантичної сегментації. Для цього автори запропонували три варіанти спеціальних модулів дистиляції:

\textcolor{red}{
\begin{itemize}
	\item ПЕРЕВЕСТИ НОРМАЛЬНО
	\item Проста конкатенація результатів;
	\item Передача повідомлень між задачами
	\item Передача повідомлень з увагою
\end{itemize}}

Подальшим розвитком цього методу стала робота PAP-Net, в якій було представлено модуль, що окремо вивчає попарні взаємозв'язки між задачами, та об'єднує вивчені карти ознак відповідно до попарних відношень.

\subsection{Багатозадачне навчання в задачах класифікації та сегментації}

\section{Аналіз методів оцінки якості класифікації та сегментації зображень в задачі автоматизованого скринінгу}

\subsection{Помилки 1 і 2 роду та матриця невідповідностей}

В задачах математичної статистики, помилка першого роду - це хибне відхилення правильної  гіпотези (хибно-позитивний результат, FP), тоді як помилка другого роду - це прийняття хибної гіпотези (хибно-негативний результат, FN). Дані поняття використовуються, коли потрібно прийняття бінарного рішення на основі деякого критерію, що може мати похибку.

В задачах контрольованого навчання, зокрема бінарної класифікації, за за нульову гіпотезу зазвичай приймається приналежність елемента вибірки до класу, що зустрічається в виборці частіше. Також, в задачах багатокласової класифікації використовується розширення поняття помилок першого і другого роду на задачу з більшою кількістю гіпотез - матриця невідповідностей. Матриця невідповідностей - це матриця, в строках якої зазначені зразки прогнозованого класу, а кожен із стовпців представляє зразки справжнього класу. Така матриця дозволяє оцінити, чи допускає система невідповідності між класами. 

\subsection{Влучність, чутливість та специфічність}

Влучність, чутливість та специфічність є похідними мірами якості бінарного класифікатора та розраховуються на основі значень в матриці невідповідностей.
Влучність (англ. precision) - вимірює частку істинно-позитивних зразків серед знайдених.

\begin{equation*}
	Precision = \frac{TP}{TP+FP}
\end{equation*}

Чутливість, або повнота (англ. sensitivity, recall) - вимірює частку істинно-позитивних зразків серед усіх позитивних зразків.


\begin{equation*}
Recall = \frac{TP}{TP+FN}
\end{equation*}


Специфічність (англ. specificity) - вимірює частку істинно-негативних зразків серед усіх негативних зразків. 

\begin{equation*}
Specificity = \frac{TN}{TN+FP}
\end{equation*}

Між влучністю і повнотою, та чутливістю і специфічністю існує обернена залежність, коли можливо підвищити одну ціною зниження іншої. Через наявність такої залежноси, на практиці, ці міри не використовуються окремо. Замість цього використовують агреговані метрики, що дозволяють оцінити якість класифікатора в цілому.

\subsection{Міра F1 та індекс подібності Соренсена}

F1 та індекс подібності Соренсена (\textit{англ. Dice score}) є еквівалентними метриками, що, зазвичай використовуються в задачах класифікації (F1) та сегментації (Dice score). 

Міра F1 визначається як середнє гармонічне між влучністю та повнотою:

\begin{equation*}
F1 = \frac{2}{precision^{-1} + recall^{-1}}=2 \cdot \frac{precision \cdot recall}{precision + recall} = \frac{2TP}{2TP+FP+FN}
\end{equation*}
 
Індекс подібності Соренсена визначається на множинах, і, в задачах сегментації, визначається на піксельних масках сегментації:

\begin{equation*}
D_c = 2 \cdot \frac{A \cap B}{|A| + |B|}
\end{equation*}

Також, для бінарних даних $A \cap B$ відповідає $TP$, а $|A| + |B|$ відповідає всій множині об'єктів, тобто еквівалентно $TP + FP + FN$. Таким чином, коефіцієнт Соренсена є еквівалентним мірі F1. 

\subsection{Каппа Коена}
Оскільки використання точності неможливе в задачах з сильним дисбалансом класів, значення точності можна перенормувати. Перенормування виконується за допомогою статистики chance adjusted index: точність класифікації нормується з використанням точності, яку можна було б отримати випадково. Під випадковою тут розуміється точність рішення, яке отримано з нашого випадковою перестановкою класів:	

\begin{equation}\label{kappa}
\kappa = 1- \frac{\sum_{i=1}^{k} \sum_{j=1}^{k}w_{ij}o_{ij}} {\sum_{i=1}^{k} \sum_{j=1}^{k}w_{ij}e_{ij}},
\end{equation}

Де $k$ - це кількість класів, $o_{ij}$, та $e_{ij}$ це елементи предсказуваних та істинних матриць. $w_{ij}$ розраховується наступним чином для квадратично-зваженої метрики:

\begin{equation}\label{kappa_weights}
w_{ij} = \frac{(i - j)^2}{(k - 1)^2},
\end{equation}

Через чутливість каппи Коена, дослідники повинні ретельно інтерпретувати цю метрику. Наприклад, якщо розглянути дві пари оцінювачів з однаковим відсотком угоди, але різними пропорціями по класах, це суттєво вплине на значення метрики.

Іншою проблемою є кількість класів: із збільшенням кількості класів, метрика підвищується. Крім того, каппа Коена може бути низькою, хоча рівень згоди високий, і окремі оцінки є точними. 

Це робить Каппу Коена мінливим коефіцієнтом для аналізу.

\subsection{Криві характеристик класифікатора: ROC и PR}


\section{Висновки та постановка задачі дослідження}