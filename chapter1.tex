\chapter{Аналіз моделей та методів обробки планарних зображень в задачах автоматизованого скринінгу}

\section{Задача автоматизованого скринінгу}
Задача автоматизованого скринінгу постає, коли виникає необхідність ідентифікації рідкісних випадків в великій популяції. Метою скринінгу є раннє виявлення аномальних екземплярів в популяції, що дозволяє забезпечити раннє реагування з розрахунку на завчасне полегшення негативних наслідків. Попри те, що скринінг сприяє ранньому реагуванню, існує можливість гіпердіагностики або помилкової діагностики, а також створення неправдивого почуття упевненості у відсутності проблеми. З цих причин скринінгові дослідження повинні мати достатню чутливість і допустимий рівень специфічності \cite{screening-def}. Також, в задачі автоматизованого скринінгу важливою є локалізація ознак можливих проблем, для подальшого оцінювання людьми. \cite{screening-seg}

Характерним для задачі скринінгу є клас планарних зображень. Особливістю планарних зображень є постійний масштаб об'єктів, та можливість знехтувати перспективними спотвореннями \cite{zissermann}. 
%
\subsection{Задачі автоматизованого скринінгу в медицині}
Серед багатьох застосувань скринінгу в медицині, особливо нагальною є потреба в методах скринінгу для агресивних прихованих хвороб в регіонах, де в більшості населення немає доступу до професійного лікаря. Так, агресивними хворобами, для яких утруднене діагностування на ранніх стадіях є діабетична ретинопатія та рак шкіри. 

\subsubsection{Скринінг раку шкіри}
Рак шкіри є найбільш розповсюдженим видом злоякісних пухлин, і саме меланома є причиною більшості смертей від раку. Світова проблема захворюваності на меланому стрімко зростала за останні 50 років і стала проблемою, з якою намагаються боротися багато вчених з різних країн. 

Меланома є п'ятим за поширеністю раком серед чоловіків та шостим за поширеністю раком серед жінок \cite{noauthor_melanoma_2012}. Подібно до інших типів раку, ранні та легкі стадії візуально навряд чи можна розрізнити. В наш час, дерматологи оцінюють кожну родинку пацієнта, щоб виявити незвичні осередки або такі, що, швидше за все, є злоякісними. Якщо меланому помітити вчасно, її можна вилікувати незначними оперативними втручаннями.

\emph{Огляд літератури}
Недавні дослідження в галузі автоматичного виявлення злоякісних утворень пов'язані з найсучаснішими підходами до глибокого навчання в розпізнаванні зображень. Набагато менше робіт використовують класичне машинне навчання та сконструйовані дослідниками ознаки. 

Тут наведено найвпливовіші роботи в цій галузі. Так, Mustafa та ін. \cite{mustafa_svm} створив підхід з підібраними вручну ознаками (GrabCut для сегментації раку) та методом опорних векторів для класифікації ракових уражень. Також, Nasiri et al. [8] згенерував похідні зображення за допомогою декількох алгоритмів і використав на них метод k-найближчих моделей сусідів для вирішення завдання.

Брінкер та ін. \cite{brinker} проводив експерименти із попередньо навченими на наборі даних ImageNet згортковими нейронними мережами, такими як ResNet-50, для класифікації ранніх стадій меланоми. Автори використали 4204 перевірених біопсією зображення меланоми та звичайних родинок. Крім того, були інтегровані новітні на той момент методи глибокого навчання: різні  темпи навчання для різних частин нейронної мережі, зменшення темпу навчання на основі функції косинуса, стохастичний градієнтний спуск з перезапуском для того щоб уникнути локальних мінімумів.

Коделла та ін. \cite{codella2016deep} запропонували систему сегментації та класифікації меланоми за дермоскопічними зображеннями шкіри. Для класифікації хвороб вони застосували ансамбль останніх методів машинного навчання, включаючи глибокі залишкові мережі, згорткові нейронні мережі тощо. Вони довели, що ансамблі здатні давати кращі результати, ніж моделі окремо.

Насірі та ін. \cite{Nasiri2020} запропонували класифікацію уражень шкіри за допомогою глибокого навчання для раннього виявлення меланоми в системі міркувань на основі прецедентів (англ. case-based reasoning). Цей підхід був використаний для отримання схожих вхідних зображень із бази даних прецедентів запропонованої системи DePicT Melanoma Deep-CLASS для підвищення точності рекомендацій щодо запитуваної проблеми (наприклад, зображення родинки). Їх метод, що заснований на глибоких згорткових нейронних мережах, генерує ознаки з зображень, щоб використовувати їх у процесі пошуку в базі даних. Інтеграція цього підходу до DePicT Melanoma CLASS значно покращила ефективність класифікації зображень та якість рекомендаційної частини системи.

Дослідження в галузі багатозадачного навчання також проводили Сонг та ін. \cite{song2020}. Вони запропонували нейронну мережу, яка може одночасно виконувати завдання детекції, класифікації та сегментації уражень шкіри, не вимагаючи додаткових етапів попередньої обробки або подальшої обробки. Подібну роботу представили Чен та співавтори \cite{chen2019}, вони використали багатозадачну мережу U-Net для задачі детекції та сегментації.
Янг та ін. \cite{yang2017novel} запропонував більш складну багатозадачну модель, яка одночасно вирішує завдання сегментації уражень та дві незалежні задачі бінарної класифікації, використовуючи спільності та відмінності між завданнями.

\subsubsection{Скринінг діабетичної ретинопатії}
Діабетична ретинопатія є одним із найбільш загрозливих ускладнень діабету, при якому пошкодження сітківки викликає сліпоту. Вона пошкоджує кровоносні судини тканини сітківки, викликаючи витік рідини та погіршення зору. Поряд із захворюваннями, що призводять до сліпоти, такими як катаракта і глаукома, ретинопатія є одним із найпоширеніших захворювань, згідно зі статистикою США, Великобританії та Сінгапуру. 

Лікарі встановили чотири стадії діабетичної ретинопатії:
\begin{itemize}
	\item Легка непроліферативна ретинопатія, найраніша стадія, коли можуть виникати лише мікроаневризми;
	\item Помірна непроліферативна ретинопатія, стадію якої можна описати втратою здатності кровоносних судин до транспортування крові через  набряк з прогресуванням захворювання;
	\item Важка непроліферативна ретинопатія призводить до обмеженого кровопостачання сітківки через підвищений набряк великої кількості кровоносних судин;
	\item Проліферативна діабетична ретинопатія - це запущена стадія, коли фактори росту, що виділяються сітківкою, активізують проліферацію нових кровоносних судин, зростаючи вздовж оболонки сітківки в склоподібному тілі, заповнюючи око.
\end{itemize}

Щонайменше 56\% нових випадків можна уникнути за допомогою належного та своєчасного лікування та скринінгу очей. Однак, початкова стадія цього захворювання не має помітних для пацієнта ознак, і виявити його на ранній стадії є справжньою проблемою. Більш того, добре навчені діагности іноді не можуть вручну оцінити стадію за діагностичними зображеннями очного дна пацієнта. 

\emph{Огляд літератури}

Багато дослідницьких зусиль було присвячено проблемі раннього виявлення діабетичної ретинопатії. Перш за все, дослідники намагалися використовувати класичні методи комп'ютерного зору та машинного навчання, щоб забезпечити відповідне рішення цієї проблеми. 

Наприклад, Прія та ін. \cite{priya} запропонували підхід на основі комп'ютерного зору для виявлення діабетичної ретинопатії за допомогою кольорових зображень очного дна. Автори створили набір ознак із вихідного зображення, використовуючи класичні методи обробки зображень, і використали метод опорних векторів для бінарної класифікації. Їх метод досяг чутливості 98\%, специфічності 96\% та точності 97\% на тестовому наборі з 250 зображень. 

Крім того, інші дослідники намагалися використати інші моделі для багатокласової класифікації, наприклад, застосовуючи аналіз головних компонент до зображень та використовуючи дерева рішень, Баєсові класифікатори або метод найближчих сусідів \cite{conde} з найкращими результатами 73.4\% точності та 68.4\% для F-міри, використовуючи набір даних із 151 зображення з різною роздільною здатністю.

Зі зростанням популярності підходів, заснованих на глибокому навчанні, з’явилися методи, які застосовують глибокі згорткові нейронні мережі до цієї проблеми. Пратт та ін. \cite{pratt} розробили архітектуру нейронної мережі та використали аугментацію даних, яка може ідентифікувати складні ознаки захворювання, пов'язані із задачею класифікації, такі як мікроаневризми, ексудат та крововиливи в сітківку ока, і, отже, автоматично діагностувати стадію захворювання. Цей метод досяг чутливості 95\% і точності 75\% на 5000 валідаційних зображень. Крім того, є й інші роботи про використання глибоких штучних нейронних мереж від інших дослідників \cite{lam, li}.

Asiri та ін. провели аналіз значної кількості доступних методів та наборів даних, висвітливши їх плюси та мінуси \cite{asiri}. Крім того, автори вказали на проблеми, які слід вирішити при розробці та вивченні ефективних та надійних алгоритмів глибокого навчання для різних проблем діагностики діабетичної ретинопатії, та звернули увагу на напрямки подальших досліджень.

Інші дослідники також намагалися здійснити трансферне навчання за допомогою згорткових нейронних мереж. Хагош та ін. \cite{hagos} спробував навчити InceptionNet V3 для класифікації 5 класів з нейронною мережею, натренованою на наборі даних ImageNet і досяг точності 90,9%. 
Сарки та ін. \cite{sarki} провів дослідження з навчання різних архітектур, зокрема ResNet50, Xception, DenseNets та VGG за допомогою попереднього навчання на наборі даних ImageNet і досяг найкращої точності 81.3\%. Обидві групи дослідників використовували набори даних, які надавали APTOS та Kaggle.


\subsection{Задачі автоматизованого скринінгу в метеорології}
Однією з найцінніших особливостей визначення кліматичної моделі Землі є поведінка хмар. Однак дослідження їх поведінки є однією з найскладніших частин, оскільки вимагає досконалого розуміння всіх процесів в атмосфері. Класифікація різних типів організації хмар допомагає покращити розуміння цих хмар, що, в свою чергу, допоможе нам побудувати кращі кліматичні моделі.

Дослідники з Інституту метеорології імені Макса Планка зібрали найбільший набір даних, що складається з приблизно 10 000 фотографій хмар в видимому спектрі з супутників Terra і Aqua MODIS. Завдяки краудсорсинговій спільноті Zooniverse, вони створили анотований набір даних, в якому хмари позначені чотирма типами масок: цукор, квітка, гравій, риба (англ. Sugar, Flower, Gravel, Fish). Однак, через недосконалу процедуру маркування фотографій, та через те, що маркування було зроблене непрофесіоналами, розмітка є частково помилковою. Так, маски сегментації класів містять багато пікселів, що належать до фону, та деякі хмари не мають масок сегментації.

\emph{Огляд літератури}
Щоб описати широкий спектр дослідницьких робіт, розглянуто роботи, що використовують як супутникові зображення в видимому спектрі, так і в інших діапазонах: мультиспектральні та інфрачервоні.

Один із методів виявлення хмар був розроблений Чжу та співавтори \cite{zhu2018}, вони запропонували метод під назвою Fmask (функція маски) для виявлення хмар та їхніх тіней на зображеннях із супутника Landsat 7. Fmask використовує підходи, засновані на правилах на основі фізичних властивостей хмар, щоб відокремити потенційні хмарні регіони від чистого неба. В якості вхідних даних метод використовує інформацію із семидіапазонних датчиків Enhanced Thematic Mapper (ETM) та Enhancet Themtic Mapper Plus (ETM+), якими обладнаний супутник Landsat. Альтернативний підхід був запропонований Харб та ін. \cite{harb2016}, автори проаналізували мультиспектральні дані середньої роздільної здатності з супутників програми CBERS. Даний алгоритм використовує набір математичних операцій над спектральними смугами, щоб покращити видимість хмар та їхніх тіней. 

Ці методи дають точні результати, але в значній мірі залежать від моделей датчиків (оскільки вони є методами, заснованими на правилах), а пропоновані рішення не є масштабованими до інших типів датчиків.

Ху та співавтори \cite{hu2015} представили більш загальне рішення, використовуючи методи комп'ютерного зору для виявлення декількох низькорівневих ознак, таких як колір, особливості текстури тощо. Для оцінки піксельних масок автори використали класичні алгоритми машинного навчання. 
Озкан та ін. \cite{okzan2018} застосували глибокі нейронні мережі (такі як Feature Pyramid Network) для сегментації хмар із низькоорбітальних RGB-зображень супутників Gokturk-2 та RASAT.

\section{Проблема зашумлення розмітки в наборах даних}
В задачах машинного навчання, поняття зашумлення розмітки (\textit{англ. label noise}) є досить розмитим і його нелегко визначити точно. В літературі цей термін використовується для позначення невідповідності розмітки істинним даним, її пошкодження, або неідеальність. 

Треба відрізняти зашумлення розмітки від зашумлення вихідних даних. Так, для множини зображень $X$ та відповідної їй множини міток $Y$ утворюються пари $(x_i, y_i)$. Зазвичай, в задачах класифікації та семантичної сегментації, множина $Y$ містить дискретні мітки $y_i$ рівня зображення, або рівня окремих пікселей відповідно. Ці мітки часто відповідають справжньому класу елемента $x_i$, але вони можуть бути піддані впливу шумового процесу перед тим, як будуть представлені алгоритму навчання.

\subsection{Природне зашумлення в наборах даних}

Відповідно до \cite{frenay2014}, ідентифікація джерел зашумлення розмітки не обов’язково важлива, коли фокус аналізу зосереджений на наслідках зашумлення. Однак, коли модель зашумлення має бути вбудована безпосередньо в алгоритм навчання, може бути важливим обрати модель, яка точно пояснює джерела та вид шуму. 

Зазвичай, природне зашумлення розмітки відбувається, коли до процесу розмітки залучені люди-анотатори. Тоді, причинами некоректної розмітки можуть бути схожі структури, недостатній досвід роботи анотаторів, випадкові помилки, а також людські упередженості. 

Автори \cite{https://arxiv.org/pdf/2003.10471.pdf} пропонують наступну класифікацію видів зашумлення розмітки:

\begin{itemize}
	\item Рівномірне зашумлення - ймовірність зміни мітки класу на мітку будь-якого іншого класу розподілена рівномірно. 
	\item Залежне від класу зашумлення - ймовірність зміни мітки класу на мітку іншого класу залежить від цих класів. 
	\item Залежне від ознак зашумлення - ймовірність зміни мітки класу на мітку іншого класу залежить від ознак об'єкта. На відміну від інших двох видів, цей майже не зустрічається в роботах через складність виконання. 
\end{itemize}

Наприклад, набір даних Imagenet, що використовується як основа для трансферного навчання в інженерній практиці, має значний рівень зашумлення розмітки, через те, що в ньому на одному зображенні може знаходитися декілька об'єктів різних класів, але мітка є тільки для одного. Автори \cite{https://arxiv.org/pdf/2101.05022.pdf} оцінюють рівень зашумлення та пропонують варіант для його корекції через розбиття зображень з декількома об'єктами на декілька зображень.


Також, в задачах семантичної сегментації, зашумлення розмітки може бути упередженим \cite{vorontsov2021annotationefficient} відповідно до людей-анотаторів, кожний з яких розмічає зображення певною мірою суб'єктивно. Особливо частою ця проблема є в задачах медичної сегментації - в деяких предметних областях навіть один і той самий анотатор може давати різну розмітку одних і тих самих зображень, в залежності від власного стану \cite{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4950770/}.


Для того, щоб оцінити приблизний рівень шуму в складних умовах для людей-анотаторів, автори \cite{https://dm.kaist.ac.kr/datasets/animal-10n/} створили спеціальний набір даних, що складається з пар класів зі схожими ознаками, наприклад, кіт та рись, або вовк та койот. Після розмітки прикладів людьми, автори використали перехресну перевірку для оцінки ймовірності зміни мітки класу в парах, яка вийшла рівною 6.44\%. Одночасно з набором даних, автори також запропонували метод відновлення правильних міток класів SELFIE.   
 





\subsection{Штучне зашумлення в наборах даних}
В процесі тестування різних методів машинного навчання в умовах зашумлення розмітки виникає потреба в тестових наборах даних, що мають незашумлену розмітку, в той час, як відповідні тренувальні набори даних мають зашумлену розмітку. Оскільки збір таких наборів даних є практично неможливим, основним варіантом отримання є додавання штучного шуму в оригінальну, якісну розмітку тільки для тренувального набору даних. 

Наприклад, в роботі \cite{https://arxiv.org/pdf/2003.10471.pdf} запропоновані методи додавання шуму в розмітку на основі зазначеної вище класифікації. Для всіх класів в наборі даних складається матриця зашумлення $N$, так що:

Для \textit{рівномірного зашумлення}: 

\begin{equation*}
	N_{ij} = 
	\begin{cases}
		p & \text{якщо } i = j \\
		\frac{1 - p}{M - 1} & \text{якщо } i \ne j 
	\end{cases}
\end{equation*} де М - кількість класів. 

Для \textit{залежного від класу зашумлення} автори пропонують використовувати матрицю невідповідностей, що створюється попередньо навченою нейронною мережею. 

Для \textit{зашумлення, залежного від ознак}, в роботі запропоновано використання претренованої нейронної мережі в якості екстрактора ознак, з подальшою ії кластеризацією та побудовою матриці невідповідностей. 

Так само, можна використовувати зазначені методи і для задачі семантичної сегментації, змінюючи мітки класу для всього об'єкту в цілому, однак, це не відповідає більшості реальних зашумлень масок сегментації.

Окрім зміни міток класів для об'єктів в цілому, маски семантичної сегментації можуть не повністю покривати об'єкти, або частково не відповідати їх контурам. Для моделювання цих недоліків розмітки, в роботі \cite{vorontsov2021annotationefficient} запропоновано зашумлення упередженого та неупередженого видів: неупереджене зашумлення складається з еластичної трансформації масок для кожного з об'єктів, в той час як упереджене передбачає константний зсув маски відносно об'єкта для всіх зображень. Такі модифікації можна розглядати як окремий випадок зашумлення, залежного від ознак для задачі класифікації, якщо прийняти семантичну сегментацію як задачу попіксельної класифікації.

Також, окремим випадком рівномірного зашумлення є відсутність масок сегментації для певних об'єктів, або їх присутність в місцях, де немає об'єктів. В такому разі рівномірне зашумлення відбувається між классами о'єктів та класом фону.
 
В деяких випадках, немає можливості отримати набір даних з реальної предметної області для оцінки методу. В такому разі, дослідники використовують генерацію синтетичних даних, відповідно до характеристик, притаманних предметній області. 

Так, автори \cite{https://arxiv.org/pdf/1502.04681.pdf} запропонували генерацію набору коротких відео-послідовностей з цифр, зо рухаються для перевірки можливостей рекурентної моделі до передбачення наступної позиції в послідовності. 

Автори \cite{https://github.com/RoshanRane/segmentation-moving-MNIST} розширили попередній набір даних на задачу семантичної сегментації з додаванням зашумлення вхідних даних. 

В інженерній роботі \cite{https://github.com/williford/seg-mnist-dataset} запропоновано параметричну модель генерації наборів даних для оцінки моделей та методів семантичної сегментації. Об'єкти даного набору даних є рукописними цифрами MNIST, а фон генерується з випадкових текстур, при цьому, будь-яке зашумлення розмітки відсутнє. 



%TODO: Аналіз методів та алгоритмів обробки зображень



\section{Моделі глибинного машинного навчання}
Глибинне машинне навчання - це частина більш широкого сімейства моделей машинного навчання, заснованих на штучних нейронних мережах (ШНМ). Методи глибинного машинного навчання моделюють високорівневі абстракції за допомогою графу з декількох шарів, що побудовані з лінійних, чи нелінійних перетворень.

Штучні нейронні мережі — це обчислювальні системи, натхнені біологічними нейронними мережами. ШНМ ґрунтується на сукупності з'єднаних вузлів, що називають штучними нейронами (аналогічно до біологічних нейронів у головному мозку тварин). Кожне з'єднання (аналогічне синапсу) між штучними нейронами може передавати сигнал від одного до іншого. Штучний нейрон, що отримує сигнал, може обробляти його, й потім сигналізувати іншим нейронам, приєднаним до нього \cite{cs231n}.

\subsection{Згорткові нейронні мережі}
Революція в розпізнавання образів була зроблена за допомогою згорткових нейронних мереж (ЗНМ). Раніше, для задач розпізнавання використовувалися фільтри, які обиралися вручну, а після них використовувався простий класифікатор. Велика перевага нейронних мереж, полягає в тому, що потрібні лише тренувальні дані. На основі даних, фільтри і класифікатори навчаються автоматично. Це стало особливо потужним методом в завданнях розпізнавання зображень.
 
Дані з зображень захоплюється за допомогою операції згортки. Використовуючи згорткові ядра для сканування цілого зображення, потрібно вивчити порівняно небагато параметрів відносно повнозв'язних штучних нейронних мереж. Окрім безпосередньо операції згортки, сучасні ЗНМ використовують нелінійні функції активації, операції підвибірки та нормалізації. 

\textcolor{red}{
\paragraph{Згорткові шари}
Згорткові шари в глибоких нейронних мережах є основним блоком, що дозволяє формувати карти ознак.
}

\paragraph{Активації}
Функції активації забезпечують нелінійність нейронної мережі, виконуючи нелінійне перетворення карти ознак, отриманої від згорткового, або повнозв'язного шару. 

Найбільш відомими функціями активації в даний час є \cite{activations_survey}:
\begin{itemize}
	\item Сигмоподібна (Sigmoid) \cite{sigmoid}
	\item Гіперболічний тангенс (TanH) \cite{tanh}
	\item Лінійний ректифікатор (ReLU) \cite{relu}
	\item Нещільний лінійний ректифікатор (LeakyReLU) \cite{leakyrelu}
	\item SoftPlus \cite{softplus}
	\item Доповнена тотожна функція (Bent identity) \cite{bidentity}
	\item Swish \cite{swish}
	\item Mish \cite{mish}
\end{itemize}

Графіки зазначених функцій зображено на рис. \ref{fig:activations}.

\begin{figure}
	\centering
	\includegraphics[width=8cm]{activations.png}
	\caption{Графіки популярних функцій активації \cite{mish}}
	\label{fig:activations}
\end{figure} 

\paragraph{Підвибірки}
Шари підвибірки використовуються для зменшення просторової роздільної здатності та агрегації просторових ознак. В сучасних нейронних мережах використовуються шари підвибірки з функцією максимуму та середнього арифметичного. Також існують шари підвибірки з іншими функціями, але вони не набули загального застосування \cite{abc, def, grh}.

\paragraph{Нормалізації}
В сучасних нейронних мережах, шари нормалізації використовуються для стабілізації процесу навчання. Розподіл входів кожного шару змінюється під час навчання, оскільки змінюються параметри попередніх шарів. Це уповільнює навчання, вимагаючи нижчих темпів навчання та ретельної ініціалізації параметрів. 

Нормалізація проводиться над картами ознак, або їх частинами, приводячи їх до розподілу з середнім значенням 0 та стандартним відхиленням 1. Різні типи нормалізацій які застосовуються в нейронних мережах включають:

\begin{itemize}
	\item Пакетна нормалізація (batch normalization)\cite{batchnorm}
	\item Пошарова нормалізація (layer normalization)\cite{layernorm}
	\item Поекземплярна нормалізація  (instance normalization) \cite{inastancenorm}
	\item Групова нормалізація (group normalization) \cite{groupnorm}
\end{itemize}

Візуалізація різних типів нормалізації зображена на рис. \ref{fig:normalizations}. Тут $N$ - кількість карт ознак в пакеті,  $C$ - розмірність каналів карти ознак, $H$, $W$ - просторова розмірність одного каналу. Синім виділено набори ознак, на яких обчислюється середнє значення та стандартне відхилення. 

\begin{figure}
	\centering
	\includegraphics[width=16cm]{normalizations.png}
	\caption{Візуалізація різних типів нормалізації \cite{groupnorm}}
	\label{fig:normalizations}
\end{figure} 

\subsection{Визначні сучасні моделі класифікації зображень на основі ШНМ}

\paragraph{Модель нейронної мережі AlexNet} 

AlexNet є однією з найперших вдалих спроб застосування штучних згорткових нейронних мереж до зображень у великому масштабі. На відміну від більш ранніх робіт, автори досягли великого прориву на великому наборі даних ImageNet \cite{imagenet}.

Архітектура нейронної мережі є логічним продовженням розробок Я. Лекуна \cite{lenet} для розпізнавання рукописних літер за допомогою згорткових нейронних мереж. AlexNet складається з восьми шарів, перші п’ять з яких - згорткові, а останні три - повнозв’язними шарами. AlexNet використовує функцію активації ReLU, яка демонструвала покращені результати навчання в порівнянні сигмоподібною. Для збільшення поля зору, автори використали великий розмір ядра в перших згорткових шарах (11 та 5 пікселів відповідно)

Для того, щоб мати можливість навчання не великій кількості зображень порівняно високої роздільної здатності, автори розробили алгоритм паралельного навчання на декількох графічних процесорах (GPU).

\begin{figure}
	\centering
	\includegraphics[width=16cm]{alexnet.png}
	\caption{Графічна репрезентація моделі нейронної мережі AlexNet \cite{alexnet}}
	\label{fig:alexnet_dag}
\end{figure} 

Також, автори вперше показали можливість використання карт ознак як міри подібності для вхідних зображень та необхідність регуляризації для запобігання перенавчанню. 

AlexNet вважається однією з найвпливовіших статей, опублікованих в галузі комп'ютерного зору, завдяки чому було опубліковано багато інших публікацій із використанням CNN та GPU для прискорення глибокого навчання. 


\paragraph{Модель нейронної мережі VGGNet}
VGGNet є інкрементальним покращенням архітектури AlexNet, в якому адресовано проблему продуктивності та систематизовано залежність точності від глибини нейронної мережі. Покращення продуктивності в порівнянні з AlexNet досягається заміною великих розмірів ядра згортки (11 і 5 у першому та другому згортковому шарі, відповідно) кількома згортками з розміром ядра 3х3 одна за одною. Також, за рахунок використання більшої кількості нелінійних активацій, підвищується дикримінаційна здатність нейронної мережі. Так, три згортки з ядром 3х3 одна за одною та кроком 1 мають той самий розмір поля зору що й одна згортка з розміром ядра 7х7, але кількість задіяних параметрів становить $3 \cdot (3^2 C^2)$ порівняно з $7^2 C^2$ параметрів для ядер розміром 7х7. Тут $C$ - це кількість вхідних та вихідних каналів згорткового шару. 

Автори порівнюють декілька нейронних мереж, які відрізняються лише кількістю шарів. Вихідна таблиця порівняння архітектур зазначена на рис. \ref{fig:vggnet_table}.

\begin{figure}
	\centering
	\includegraphics[width=16cm]{vgg.png}
	\caption{Порівняння моделей VGGNet з різною кількістю шарів \cite{vgg}}
	\label{fig:vggnet_table}
\end{figure} 

В даній роботі було продемонстровано, що глибина нейронної мережі вигідна для точності класифікації, а також того, що можна досягти високої точності за допомогою звичайної архітектури згорткової нейронної мережі. 

\paragraph{Сімейство моделей нейронних мереж Inception}
Мережа Inception \cite{inception}стала важливою віхою в розвитку згорткових нейронних мереж. До цієї роботи, згорткові нейронні мережі являли собою послідовні операції згортки, активації та підвибірки. До сімейства Inception входять нейронні мережі, що були спеціально розроблені для підвищення точності та продуктивності. 

Автори розглядають проблему варіативності розміру об'єктів на зображеннях: вибір правильного розміру ядра для операції згортки стає важким. Більше ядро краще підходить для виділення інформації, яка розповсюджена більш глобально, а менше ядро - для інформації, яка розповсюджується локально.

Для вирішення цієї проблеми, автори використовують декілька операцій згортки з різними розмірами ядра в одному шарі нейронної мережі. Оскільки виконання згорток з великою кількістю каналів є довгою операцією, автори зменшують кількість каналів за допомогою згорток з розміром ядра 1х1. Один такий шар називається \textit{блоком Inception}, архітектура якого  зображена на рис. \ref{fig:naive_inception}

\begin{figure}
	\centering
	\includegraphics[width=8cm]{inception_v1_1.png}		\includegraphics[width=8cm]{inception_v1_2.png}
	\caption{Блок-схема Inception v1 до та після розкладу\cite{inception}}
	\label{fig:naive_inception}
\end{figure} 

Нейронна мережа GoogLeNet \cite{inception_v1} складалася з 22 шарів в глибину та була найглибшою на той час. Для того, щоб запобігти затуханню градієнтів, використовувалися додаткові класифікатори на менш глибоких рівнях, результати яких було додано до функції втрат.

В наступних роботах \cite{inception_v2}, автори продовжують ідею зменшення кількості операцій, для чого вводять розкладання згорток з великим розміром ядра на декілька менших згорток. Так, автори пропонують розкладання згортки з ядром розміру 5х5 на дві згортки з розміром ядра 3х3, а згортки з розміром ядра 3х3 на дві згортки розміру 1х3 та 3х1 відповідно. Це дозволяє зменшити кількість параметрів в нейронній мережі при збереженні глибини, зменшуючи необхідні ресурси для обчислення.

Схема блоків Inception v2 та v3 зображено на рис. \ref{fig:inception_v2}
\begin{figure}
	\centering
	\includegraphics[width=8cm]{inception_v2_1.png}		\includegraphics[width=8cm]{inception_v2_2.png}
	\caption{Блок-схема Inception v2 до та після розкладу\cite{inception_v2}}
	\label{fig:inception_v2}
\end{figure} 

В нейронній мережі Inception v2 вперше було застосовано метод пакетної нормалізації \cite{batchnorm}, що дозволило значно пришвидшити навчання. Також, було показано, що додаткові класифікатори не мають впливу на затухання градієнтів, але мають властивості регуляризації, особливо на останніх стадіях навчання.

\paragraph{Сімейство моделей нейронних мереж ResNet}

Сімейство нейронних мереж ResNet було засновано на новому методі залишкового навчання, що був представлений в тій самій роботі, що й архітектура нейронної мережі \cite{resnet}. Автори розв'язують проблему того, що збільшення глибини нейронної мережі зменшує точність як на навчальному, так і на тестувальному наборі даних. 

В роботі показано, що метод стохастичного градієнтного спуску не придатний до оптимізації глибоких нейронних мереж. Так, наведено експеримент, в якому між шарами неглибокої нейронної мережі використано шари тотожного відображення або додаткові згорткові шари з функцією активації. Показано, що для більш глибокої нейронної мережі має існувати рішення з точністю не меншою, ніж для неглибокої нейронної мережі, але, на практиці, точність більш глибокої нейронної мережі залишається нижчою. Для збільшення глибини нейронних мереж без втрати точності, в роботі запропоновано метод залишкового навчання. 

Нехай $\mathcal{H}_l(x_{l-1})$ - це відображення, що задається декількома шарами нейронної мережі, де $x_{l-1}$ - це вхідний тензор до першого з цих шарів. Якщо припустити, що декілька нелінійних шарів можуть апроксимувати будь-яку нелінійну функцію, то логічно припустити, що вони можуть апроксимувати і залишкову функцію: $\mathcal{F}_l(x_{l-1})=\mathcal{H}_l(x_{l-1}) - x_{l-1}$. Тоді, оригінальне відображення стає $\mathcal{H}_l(x_{l-1})=\mathcal{F}_l(x_{l-1}) + x_{l-1}$. Хоча ці відображення є еквівалентними, алгоритму оптимізації <<простіше>> таким чином вивчити тотожну функцію, оскільки для цього достатньо наблизити ваги шарів нейронної мережі до нуля.

В роботі автори пропонують блоки нейронних мереж, що складаються з декількох згорткових шарів з функцією активації ReLU та об'єднання з вхідним тензором першого шару. Схема таких блоків зображена на рис. \ref{fig:res_block}

\begin{figure}
	\centering
	\includegraphics[width=10cm]{res_blocks.png}
	\caption{Схема залишкових блоків нейронних мереж ResNet18 та ResNet50 \cite{resnet}}
	\label{fig:res_block}
\end{figure} 

Завдяки використанню таких блоків, стала можливою побудова надзвичайно глибоких нейронних мереж. В експериментах, автори запропонували архітектури, що складаються з 18, 34, 50, 101, 152 та 1202 згорткових шарів, та можуть бути оптимізовані стандартними методами (наприклад, методом стохастичного градієнтного спуску).



\paragraph{Сімейство моделей нейронних мереж DenseNet}

Архітектура нейронних мереж сімейства DenseNet \cite{densenet} є логічним продовженням ідей, запропонованих в архітектурі ResNet. В даній архітектурі також використовується метод залишкового навчання, але, на відміну від архітектури ResNet, послідовні шари поєднані кожний з кожним. 

Для того, щоб підвищити об'єм інформації, яким обмінюються шари нейронної мережі, автори пропонують ввести пряме з'єднання від кожного шару до всіх наступних:
\begin{equation}
	x_l = \mathcal{H}([x_0, x_1, x_3 ... x_{l-1}])
\end{equation}
де $[x_0, x_1, x_3 ... x_{l-1}]$ - конкатенація всіх карт активації з шарів від $0$ до $l-1$. 

Така зміна архітектури дозволила останнім шарам перевикористовувати ознаки з попередніх, що, в свою чергу, дозволило зменишити кількість параметрів в нейронній мережі та підвищити точність.

Граф нейронної мережі DenseNet зображено на рис. \ref{fig:densenet}.

\begin{figure}
	\centering
	\includegraphics[width=16cm]{densenet.png}
	\caption{Графічна репрезентація моделі нейронної мережі DenseNet \cite{densenet}}
	\label{fig:densenet}
\end{figure} 

На практиці, хоча архітектура дозволяє досягти більшої точності з використанням меншої кількості параметрів, на час виходу роботи, більшість реалізацій, заснованих на популярних фреймворках машинного навчання вимагали більшої кількості оперативної пам'яті через необхідність копіювання карт активацій для кожного з шарів. Таким чином, наївна реалізація архітектури DenseNet потребувала $\mathcal{O}(\frac{L(L-1)}{2})$ оперативної пам'яті,  де $L$ - кількість шарів . З розвитком фреймворків машинного навчання, була реалізована можливість перевикористання тензорів без копіювання, що дозволило знизити потребу оперативної пам'яті до $\mathcal{O}(L)$ \cite{efficient_densenet}.


\paragraph{Сімейство моделей нейронних мереж ResNeXt}
Архітектури сімейства ResNeXt \cite{resnext} є логічним продовженням методів, застосованих в архітектурах Inception та ResNet. Основною ідеєю цієї архітектури є використання набору з декількох простих однакових трансформацій в просторі низької розмірності в кожному з шарів нейронної мережі. 

Так, автори пропонують розділити згортковий шар на декілька паралельних трансформацій, результати з яких агрегуються:

\begin{equation}
	\mathcal{H}(x_{l-1}) = \sum_{i=1}^{C} \mathcal{T}_i(x_{l-1})
\end{equation}

де $\mathcal{T}_i$ - це будь-яка нелінійна функція, що проектує вхід $x_{l-1}$ до простору низької розмірності, трансформує його, та проектує результат назад в простір високої розмірності. Тут $C$ - потужність множини трансформацій. 

Графічну репрезентацію блоку ResNeXt зображено на рис. \ref{fig:resnext}

\begin{figure}
	\centering
	\includegraphics[width=12cm]{resnext.png}
	\caption{Графічна репрезентація блоку ResNeXt \cite{resnext}}
	\label{fig:resnext}
\end{figure} 

В роботі автори проводять експерименти відносно важливості параметру $C$ відносно інших, та показують, що збільшення $C$ підвищує точність більше, ніж підвищення кількості каналів згорток. Також, в роботі показано, що такий метод здатний створювати кращі репрезентаціїї вхідних даних.

\paragraph{Сімейство моделей нейронних мереж EfficientNet}
Сімейство нейронних мереж EfficientNet також є логічним продовженням архітектури ResNet. В даній роботі вперше розглядається параметрична модель та узагальнений метод масштабування згорткових нейронних мереж для збільшення точності під заданий обчислювальні можливості. 

Автори пропонують метод, за допомогою якого можна масштабувати одночасно вхідну роздільну здатність зображення, глибину та ширину нейронної мережі. Наведено спостереження, що ці величини не є незалежними.

Інтуїтивно зрозуміло, що для зображень із більш високою роздільною здатністю необхідно збільшити глибину мережі, щоб більше збільшити поле зору та захопити більше пікселів у більшому зображенні. Відповідно, також необхідно збільшувати ширину мережі, коли роздільна здатність збільшується, для того, щоб мати можливість розпізнати більшу кількість ознак на зображеннях із високою роздільною здатністю. 

Для цього, модель нейронної мережі формулюється як задача максимізації точності в залежності від ширини, глибини та роздільної здатності:

\begin{align}
& max \; Accuracy(\mathcal{N}(d, w, r)):
\\
& \mathcal{N}(d, w, r) = \bigodot_{i=1..s} \hat{\mathcal{F}}_i^{d \cdot \hat{L_i}}(X_{(r \cdot \hat{H_i}, r \cdot \hat{W_i}, w \cdot \hat{C_i)}})
\end{align}

де $w, d, r$ - параметри масштабування ширини, глибини та роздільної здатності відповідно,

та $\hat{\mathcal{F}}_i$ - функція, що задається моделлю шару,

$\hat{L}_i$, $\hat{H}_i,\hat{W}_i$ - розміри карти активації, а $\hat{C}_i$ - кількість каналів. 

Також, пропонується визначення залежності між $d, w,  r$: 

\begin{align}
& d = \alpha^\phi
\\
& w = \beta^\phi
\\
& r = \gamma^\phi
\end{align}

так, що виконується наступне:

\begin{align}
&\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2
\\
&\alpha \geq 1, \beta \geq 1, \gamma \geq 1
\end{align}

де $\phi$ - множник, що задається в залежності від наявного обчислювального бюджету.

Завдяки цьому методу масштабування, автори представляють 8 варіантів нейронної мережі, що відрізняється необхідною точністю та продуктивністю: від найменшої до найбільшої. 

\subsection{Визначні моделі ШНМ для сегментації зображень}
\paragraph{Повністю згорткова нейронна мережа}
(\textit{англ. Fully-convolutional network, FCN}) \cite{fcn} - це перша нейронна мережа, що використовувала лише шари згортки, активації та підвибірки, не використовуючи повнозв'язних шарів в задачі семантичної сегментації.

Основною перевагою цього підходу стала можливість навчання та прогнозування на зображеннях різного розміру, без зміни параметрів нейронної мережі. 

В даній роботі вперше було застосовано метод трансферного навчання з задачі класифікації на задачу сегментації. Для цього перші шари нейронної мережі було ініціалізовано за допомогою параметрів, отриманих після тренування на наборі даних ImageNet \cite{imagenet}. 

Для отримання масок сегментації, повнозв'язні шари було сконвертовано в згорткові, а також було додано інформацію з перших шарів для збереження просторових ознак. Граф нейронної мережі FCN зображено на рис. \ref{fig:fcn_dag}.

\begin{figure}
	\centering
	\includegraphics[width=16cm]{fcn.png}
	\caption{Графічна репрезентація моделі нейронної мережі FCN \cite{fcn}}
	\label{fig:fcn_dag}
\end{figure} 

Тут, \textit{pool[1-5]} - карти активації після відповідних операцій підвибірки, а \textit{upsampled} - карти активації після білінійної інтерполяції. 


\paragraph{Модель нейронної мережі SegNet}

SegNet (\textit{сегментаційна мережа}) \cite{segnet}- перша повністю згорткова нейронна мережа, що використовувала архітектуру типу "енкодер-деокдер" для вирішення задачі семантичної сегментації. В даній архітектурі, енкодер стискає семантичне представлення до більш компактного простору. Роль декодера полягає у відображенні з компактного простору з низькою роздільною здатністю у карту актавації повного розміру для класифікації кожного пікселя.

Архітектура мережі енкодера топологічно ідентична 13 згортковим шарам мережі VGG16 \cite{vgg}.

Новина SegNet полягає в тому, як декодер підвищує розширення карт активаціїї низького розширення. Так, декодер використовує індекси операцій підвибірки, обчислені на відповідному шарі енкодера, для нелінійного підвищення розширення. Карти активації підвищенного розлирення виявляються розрідженими, для того, щоб зробити їх щільними, виконується декілька операцій згортки. Структура SegNet зображена на рисунку \ref{segnet_dag}.

\begin{figure}
	\centering
	\includegraphics[width=16cm]{segnet.png}
	\caption{Граф нейронної мережі SegNet \cite{segnet}}
	\label{fig:segnet_dag}
\end{figure} 

\paragraph{Модель нейронної мережі PSPNet}
Pyramid Scene Parsing Network (PSPNet) - це подальший розвиток ідеї FCN, в якому виправлено декілька значних проблем архітектури FCN. 
Серед основних проблем FCN автори відзначили:

\begin{itemize}
	\item Відсутність можливості обробки інформації про контекст рівня зображення
	\item Різний розмір та вигляд різних об'єктів одного класу
\end{itemize}

Для того, щоб вирішити ці проблеми, автори запропонували додати до архітектури FCN модуль пірамідальної підвибірки (\textit{англ. Pyramid Pooling Module }). Цей модуль складається з чотирьох операцій підвибірки з різними масштабом, що дозволяє захопити контекст зображення на різних рівнях. 

Структура PSPNet та модуля підвибірки зображена на рисунку \ref{pspnet_dag}.


\begin{figure}
	\centering
	\includegraphics[width=16cm]{pspnet.png}
	\caption{Граф нейронної мережі PSPNet \cite{pspnet}}
	\label{fig:pspnet_dag}
\end{figure} 

\paragraph{Модель нейронної мережі UNet}

U-Net (\textit{U-подібна мережа}) \cite{unet} - повністю згорткова нейронна мережа, що складається з симетричних енкодера та декодера, в яких, на відміну від SegNet замість індексів підвибірки, до декодера передаються карти активації енкодера напряму, через операцію конкатенації. Окрім спрощення кодування, така організація мережі дозволила спростити навчання, оскільки передча інформації стала повністю диференційованою. 

Також, для забезпечення можливості навчання на малій кількості даних, використовується велика кількість різноманіних аугментацій вхідних зображень, щоб синтетично збільшити розмір набору даних. 

Для більш чіткого розподілення близько розташованих об'єктів, автори запропонували спеціальну зважену функцію втрат, що забезпечувала більш чітке формування границь об'єктів. 

Хоча, оригінально ця архітектура була запропонована для семантичної сегментації планарних мікроскопічних знімків, подальші дослідження показали її застосовність в широкому спектрі задач семантичної сегментації. 

Структура нейронної мережі U-Net зображена на рисунку \ref{fig:unet_dag}.

\begin{figure}[H]
	\centering
	\includegraphics[width=14cm]{unet.png}
	\caption{Граф нейронної мережі U-Net \cite{unet}}
	\label{fig:unet_dag}
\end{figure} 

\paragraph{Модель нейронної мережі LinkNet}
Архітектура нейронної мережі LinkNet є подальшим розвитком архітектури U-Net, і також складається з енкодера та декодера, що поєднані через проміжні карти активації. 

На відміну від U-Net, LinkNet використовує ResNet в якості енкодера, а також для поєднання карт активації енкодера та декодера використовує операцію суми, замість конкатенації. Це дозволило значно зменшити кількість параметрів, підвищити швидкість тренування при збереженні точності сегментації. 

Також, автори LinkNet вперше використали індуктивний переніс з ImageNet виключно до енкодера, залишивши декодер випадково ініціалізованим на момент початку навчання. 

Структура нейронної мережі LinkNet зображена на рисунку \ref{fig:linknet_dag}.

\begin{figure}
	\centering
	\includegraphics[width=10cm]{linknet.png}
	\caption{Граф нейронної мережі LinkNet \cite{linknetnet}}
	\label{fig:linknet_dag}
\end{figure} 

В подальших роботах, автори зазвичай називають свої архітектури U-Net-подібними, хоча, насправді, вони є ближчими до LinkNet. \cite{smth, smth2, smth3}

\paragraph{Модель нейронної мережі Feature Pyramid Network}
Вперше розроблена для задач детекції об'єктів, Feature Pyramid Network (FPN) \cite{fpn_original}, була успішно застосована для задач семантичної сегментації \cite{fpn_segmentation}.

Структура FPN складається з двох шляхів: на одному з них зменшується просторова та розмірність збільшується глибина карт ознак (шлях знизу вверх), на іншому - навпаки (шлях зверху-вниз). Шлях знизу-вверх представляє собою звичайну згорткову нейронну мережу (зазвичай, з сімейства ResNet). Шлях зверху-вниз представляє собою послідовність згорток та операцій підвищення розширення. 

Для отримання карти активації, з якої буде отримано результат семантичної сегментації, всі рівні декодера оброблюються двома операціями згортки, після чого приводяться до однакового просторового розширення. З цієї карти ознак отримується фінальна карта сегментаціїї за допомогою операції згортки.

Структура нейронної мережі FPN для семантичної сегментації зображена на рисунку \ref{fig:fpn_dag}.

\begin{figure}
	\centering
	\includegraphics[width=17cm]{fpn_seg.png}
	\caption{Граф нейронної мережі FPN \cite{fpn_segmentation}}
	\label{fig:fpn_dag}
\end{figure} 

\paragraph{Сімейство моделей нейронних мереж DeepLab}
Нейронні мережі з сімейства DeepLab \cite{deeplabv2, deeplabv3} побудовані навколо одного алгоритму підвибірки з розширеної просторової піраміди (\textit{англ. Atrous Spatial Pyramid Pooling (ASPP)} та архітектури eнкодер-декодер. ASPP - це модуль для передискретизації карт активації в декількох масштабах до виконання операції згортки. Це дозволяє за одну операцію згортки виділити ознаки в різних масштабах, а також корисний контекст зображення. 

Замість того, щоб насправді передискретизовувати карти ознак як в PSPNet, зміна масштабу виконується за допомогою фільтрів згортки різного розширення.

Структура блоку ASPP зображена на рисунку \ref{fig:aspp_block}.

\begin{figure}
	\centering
	\includegraphics[width=14cm]{aspp_block.png}
	\caption{Структура блоку ASPP в нейронних мережах DeepLab \cite{deeplabv2}}
	\label{fig:aspp_block}
\end{figure} 


\section{Методи багатозадачного глибинного машинного навчання}
Багатозадачне машинне навчання - це один з підходів машинного навчання, в якому одночасно вирішуються кілька навчальних задач, використовуючи спільності та відмінності між ними. Кожна з таких задач може бути загальною навчальною задачею, такою як навчання з учителем (наприклад, задача класифікації, або регресії), навчання без вчителя (кластеризація), напівавтоматичне навчання, завдання навчання з підкріпленням, або моделі на графах. Передбачається, що всі ці навчальні задачі або хоча б частина з них пов'язані одна з одною. 

В цьому випадку виявлено \cite{caruana}, що спільне навчання на цих задачах може призвести до значного підвищення продуктивності в порівнянні з навчанням на кожній задачі окремо. Отже, багатозадачне навчання направлено на підвищення якості узагальнення кількох пов'язаних задач. Далі будуть розглядатися виключно задачі контрольованого навчання. 

Подібно до людського навчання, корисно вивчати кілька навчальних задач разом, оскільки знання, що містяться в одній задачі, можуть бути використані іншими. Наприклад, людина, що вивчає математику та математичну статистику, може використати цей досвід для вивчення інших схожих галузей, наприклад, машинного навчання.

\subsection{Визначення багатозадачного машинного навчання}
Для n навчальних задач ${T_1, T_2, … T_n}$, де всі задачі або їх підмножина пов’язані, багатозадачне навчання (БЗМН) має на меті допомогти покращити вивчення моделі для $T_i$, використовуючи знання, що містяться в усіх n або деяких з них задчах.

В літературі визначається декілька видів БЗМН:

\emph{Однорідне БЗМН}: Для кожної з задач прогнозується лише один вихід. Наприклад, розпізнавання цифр MNIST зазвичай використовується для оцінки алгоритмів БЗМН. В цьому разі, воно розглядається як 10 завдань бінарної класифікації.

\emph{Неоднорідне БЗМН}: Для кожної з задач прогнозується свій набір виходів. Наприклад, сучасні задачі детектування об'єктів потребують одночасного прогнозування класу об'єкта (задача класифікації), його положення на зображенні та розмірів (задача регресії).
Для задач контрольованого навчання, кожна задача $T_i$ містить в собі набор даних $D_i$, що складається з $m$ елементів…

Дослідники вважають, що коли різні задачі використовують однакові набори вхідних даних, БЗМН зводиться до задачі класифікації або регресії з кількома виходами. Однак, новіші дослідження визначають БЗМН на одному наборі вхідних даних, або незалежно від кількості наборів вхідних даних.

В методах БЗМН, заснованих на глибоких нейронних мережах, виділяють дві групи відповідно до способу розподілу параметрів між різними задачами. 

\subsection{Трансферне навчання}
Трансферне навчання – це проблема машинного навчання, яка фокусується на збереженні знань, отриманих під час вирішення однієї задачі з подальшим застосуванням її результатів до іншої, але близької до неї. З практичної точки зору, повторне використання або передача інформації з раніше засвоєних завдань для вивчення нових може значно підвищити ефективність їх розв’язання [savchenko]. Деякі дослідники [] вважають багатозадачне машинне навчання підмножиною трансферного навчання.

На відміну від трансферного навчання, в багатозадачному навчанні немає різниці між задачами, та завдання стоїть підвищити продуктивність на всіх задачах одночасно.

\subsection{Жорсткий розподіл параметрів}
Зазвичай, архітектури згорткових нейронних мереж для багатозадачного навчання складаються зі спільного енкодера, який виділяє ознаки на вхідному зображенні (shared features), та окремих згорткових і/або повнозв’язних шарів для кожної з задач. 

В найпершій роботі \cite{deepfacealign}, що використовує цей принцип, було запропоновано архітектуру нейронної мережі Task-Constrained Deep Convolutional Network (TCDCN) для детекції ключових точок лиця використати додаткові задачі регресії пози голови та класифікації атрибутів. Архітектуру TCDCN зображено на рис. \ref{fig:deepfacealign}. 

\begin{figure}
	\centering
	\includegraphics[width=16cm]{deepfacealign.png}
	\caption{Архітектура нейронної мережі TCDCN \cite{deepfacealign}}
	\label{fig:deepfacealign}
\end{figure} 

Модифікацією попереднього підходу є багатозадачні каскадні мережі \ref{mcdcn}, в яких результати одних задач використовуються як доповнення до вхідної інформації для наступних. Така організація шарів дозволяє підвищити точність на більш складних завданнях при правильному підборі черги задач за складністю. Загальну архітектуру зображено на рис. \ref{fig:mcn}. Схожий підхід також використовується в деяких архітектурах нейронних мереж для розв'язання задачі детекції об'єктів \cite{rcnn, faster_rcnn, mask_rcnn}

\begin{figure}
	\centering
	\includegraphics[width=8cm]{mcn.png}
	\caption{Архітектура багатозадачної каскадної мережі \cite{mcdcn}}
	\label{fig:mcn}
\end{figure} 

Також, можлива інтеграція модулів уваги як до енкодера, так і до специфічних до задачі шарів. Таким чином, обчислення ознак відбувається не лише за рахунок параметрів енкодера, а і через специфічні до задачі модулі, що розташовані всередині мережі. Це дозволяє покращити репрезентації ознак для конкретних задач.

Можливою проблемою при багатозадачному навчанні на семантично-віддалених задачах є конфлікт градієнтів \cite{gradient_conflict}: градієнти для різних задач мають протилежні напрямки. Для розв'язання цієї проблеми було запропоновано декілька підходів \cite{gradient_conflict1, gradient_conflict2, gradient_conflict}, найпростішим з яких є навчання на задачах, що не викликають конфлікту \cite{gradient_conflict}.
 
\subsection{М’який розподіл параметрів}
Одним з варіантів архітектур, що менш схильні до утворення конфліктних градієнтів є методи архітектури з м'яким розподілом параметрів.

В методах, що використовують м'який розподіл параметрів використовується декілька енкодерів для кожної з задач, між шарами яких відбувається обмін інформацією.

Наприклад, в роботі Cross-stitch networks (перехресні мережі)\cite{crossstitch} представлена архітектура багатозадачної нейронної мережі, в якій кожній задачі відповідає окремий енкодер, але входом кожного шару кожного з енкодерів є лінійна комбінація виходів попереднього шару всіх енкодерів. Коефіцієнти лінійної комбінації також вивчаються за допомогою градієнтного спуску разом з іншими параметрами нейронної мережі. На рисунку \ref{fig:cross_stitch_block} зображено архітектуру блоку лінійної комбінації для шарів двох нейронних мереж.

\begin{figure}
	\centering
	\includegraphics[width=8cm]{cross_stitch_block.png}
	\caption{Архітектура cross-stitch блоку \cite{crossstitch}}
	\label{fig:cross_stitch_block}
\end{figure} 

Узагальненням ідеї переносу інформації між послідовними шарами декількох специфічних до задачі нейронних мереж є шлюзові мережі (\emph{англ. Sluice networks}). Вхід до кожного з шарів також є лінійної комбінацією виходів попередніх шарів для кожної з задач. На відміну від перехресних мереж, кожний шар нейронної мережі розділений на окремі підпростори: для конкретної задачі та спільний між всіма задачами. Також, додається обмеження на ортогональність підпросторів через додавання функції втрат, що мінімізує квадратовану норму Фробеніуса (матричну $L^2$ норму) між підпросторами в кожному з шарів.

Подальший розвиток шлюзових мереж було запропоновано в роботі <<Нейромережеве дискримінтивне зменшення розмірності>>(\emph{англ. Neural Discriminative Dimensionality Reduction, NDDR}) \cite{nddr}. Тут, замість використання лінійної комбінації шарів між різними задачами, виконується згортка конкатенованих виходів шарів всіх задач з ядром розміру 1х1 з виконанням подальшої нормалізації. Архітектуру NDDR зображено на рисунку \ref{fig:nddr}. 

\begin{figure}
	\centering
	\includegraphics[width=12cm]{nddr.png}
	\caption{Архітектура згорткової нейронної мережі NDDR \cite{nddr}}
	\label{fig:nddr}
\end{figure} 

Суттєвим недоліком методів, що використовують м'який розподіл параметрів є постійне збільшення кількості параметрів нейронної мережі, що, в свою чергу, збільшує необхідну кількість ресурсів для роботи такої нейронної мережі зі збереженням продуктивності. 

\subsection{Дистиляція прогнозів моделі}
Дистиляція прогнозів замість вивчення ознак, спільних до декількох задач, базується на використанні результатів для одних задач, як засобу вивчення ознак для інших. Наприклад, при одночасному вивченні задач класифікації та сегментації, карти сегментації можуть бути використані для подальшої класифікації зображення.

Першою роботою, що використала цей принцип була PAD-Net (Prediction-and-Distillation Network) \cite{padnet}. В ній використовувалися результати задач семантичної сегментації, регресії карти глибини, регресії карти поверхневих нормалей та детекції контурів для отримання уточнених результатів в задачах регресії карти глибини та семантичної сегментації. Для цього автори запропонували три варіанти спеціальних модулів дистиляції:

\begin{itemize}
	\item Проста конкатенація результатів;
	\item Передача повідомлень між задачами
	\item Передача повідомлень з вагами
\end{itemize}

Подальшим розвитком цього методу стала робота PAP-Net, в якій було представлено модуль, що окремо вивчає попарні взаємозв'язки між задачами, та об'єднує вивчені карти ознак відповідно до попарних відношень.

\subsection{Багатозадачне навчання в задачах класифікації та сегментації}

Використання багатозадачного навчання в задачах класифікації та сегментації не є поширеним в літературі. Зазвичай, в роботах, що пропонують вирішення задачі сегментації, задача класифікації вирішується на етапі попередньої обробки зображень, щоб відсіяти хибнопозитивні результати сегментації та зменшити час роботи системи в цілому через використання легких моделей класифікації.
Недоліком такого підходу є можливість поширення помилки від класифікатора, що в задачах скринінгу вимагає підвищення повноти класифікатора, з подальшою роботою сегментаційної мережі. Також, в задачах, що мають маленькі об'єкти на зображеннях, чи незбалансовані набори даних, такий підхід може зменшити влучність роботи системи порівняно з її частинами.

На відміну від послідовної обробки, в роботі SegTHOR одночасно розв'язуються задачі класифікації та сегментації, доповнюючи одна одну. Так, автори пропонують нову архітектура нейронної мережі, що заснована на архітектурі UNet, але має додатковий вихід класифікації з останнього шару мережі сегментації. Отримання глобальних класів з виходу сегментації досягається через середнє арифметичне всіх пікселів для кожного з класів. Архітектура нейронної мережі SegTHOR зображена на рисунку \ref{fig:segthor}.

\begin{figure}
	\centering
	\includegraphics[width=9cm]{segthor.png}
	\caption{Архітектура методу SegTHOR \cite{segthor}}
	\label{fig:segthor}
\end{figure} 

В нейронній мережі SegTHOR вихід класифікації використовується лише при тренуванні в якості додаткової задачі, та не використовується під час прогнозування масок сегментації.

В роботі \cite{strange_mtl_for_melanoma} автори запропонували застосувати підхід багатозадачного машинного навчання до класифікації та сегментації новоутворень шкіри. На відміну від звичайних архітектур для сегментації зображень, в роботі запропоновано сегментацію будь-якого новоутворення, з паралельною класифікацією меланоми та себорейного кератозу як окремих задач. Автори показують, що така архітектура нейронної мережі незначно покращує достовірність як в задачах класифікації, так і сегментації. В даній архітектурі під час прогнозування результати різних задач використовуються окремо.

Автори \cite{mammography} запропонували одночасно вивчати задачі класифікації та сегментації для локалізації ракових пухлин в мамографії. Як і \cite{strange_mtl_for_melanoma}, в даній роботі використовується нейронна мережа, що складається з одного енкодера і двох декодерів - для класифікації та сегментації відповідно. Архітектуру нейронної мережі зображено на рисунку \ref{mammography}. 

\begin{figure}
	\centering
	\includegraphics[width=14cm]{mammography.png}
	\caption{Архітектура нейронної мережі класифікації та сегментації новоутворень на мамографії \cite{mammography}}
	\label{fig:mammography}
\end{figure} 

В роботі демонструється, що, на відміну від двох нейронних мереж, що вивчають задачі незалежно, запропонована архітектура має більшу точність як класифікації, так і сегментації. В даній архітектурі під час прогнозування результати різних задач використовуються окремо.

Окрім підходів з жорстким розподілом параметрів, в задачі сегментації може використовуватися каскадна схема. В роботі \cite{arxiv:1709.05932} запропоновано архітектуру каскадної нейронної мережі для розв'язання задачі сегментації будівель на аерофотознімках. Для покращення контурів сегментації, в роботі вводиться нова задача регресії піксельної відстані до найближчого контуру. Результат виконання задачі регресії використовується як частина вхідних даних для задачі сегментації. Архітектуру нейронної мережі зображено на рисунку \ref{fig:arxiv:1709.05932}. Автори показують, що таке використання декількох задач дозволяє підвищити достовірність сегментації в цілому та якість контурів окремо. 

\begin{figure}
	\centering
	\includegraphics[width=14cm]{1709_05932.png}
	\caption{Архітектура нейронної мережі для сегментації будівель \cite{arxiv:1709.05932}}
	\label{fig:arxiv:1709.05932}
\end{figure} 

Деякі методи машинного навчання з частковим залученням вчителя використовують архітектури, натхнені багатозадачним машинним навчанням. Так, популярною задачею машинного навчання з частковим залученням вчителя є семантична сегментація зображень з використанням лише міток рівня зображення, точок, або обмежувальних коробок навколо об'єктів. Наприклад, в роботі \cite{google_em} запропоновано EM-алгоритм сумісно з глибокими нейронними мережами для розв'язання задачі сегментації через тренування нейронної мережі з мітками рівня зображення, або обмежувальних коробок. Задача класифікації в даному випадку використовується на етапі оцінювання очікування в ЕМ алгоритмі як апріорна інформація.  

Альтернативний метод сегментації з частковим залученням вчителя базується на функції втрат, що дозволяє одночасно оптимізувати наступні задачі: 
\begin{itemize}
	\item регресію локалізації об'єктів
	\item розширення локацій об'єктів відповідно до класів, присутніх на зображеннях
	\item обмеження границь масок сегментації об'єктів до границь об'єктів
\end{itemize}

Архітектура поєднання задач зображена на рисунку \ref{fig:sec_cnn}.

\begin{figure}
	\centering
	\includegraphics[width=16cm]{sec_cnn.png}
	\caption{Архітектура методу SEC-CNN \cite{sec_cnn}}
	\label{fig:sec_cnn}
\end{figure} 


\section{Методи оцінки якості класифікації та сегментації зображень в задачі автоматизованого скринінгу}

\subsection{Помилки 1 і 2 роду та матриця невідповідностей}

В задачах математичної статистики, помилка першого роду - це хибне відхилення правильної  гіпотези (хибно-позитивний результат, FP), тоді як помилка другого роду - це прийняття хибної гіпотези (хибно-негативний результат, FN). Дані поняття використовуються, коли потрібно прийняття бінарного рішення на основі деякого критерію, що може мати похибку.

В задачах контрольованого навчання, зокрема бінарної класифікації, за за нульову гіпотезу зазвичай приймається приналежність елемента вибірки до класу, що зустрічається в виборці частіше. Також, в задачах багатокласової класифікації використовується розширення поняття помилок першого і другого роду на задачу з більшою кількістю гіпотез - матриця невідповідностей. Матриця невідповідностей - це матриця, в строках якої зазначені зразки прогнозованого класу, а кожен із стовпців представляє зразки справжнього класу. Така матриця дозволяє оцінити, чи допускає система невідповідності між класами. 

\subsection{Влучність, чутливість та специфічність}

Влучність, чутливість та специфічність є похідними мірами якості бінарного класифікатора та розраховуються на основі значень в матриці невідповідностей.
Влучність (англ. precision) - вимірює частку істинно-позитивних зразків серед знайдених.

\begin{equation}
	Precision = \frac{TP}{TP+FP}
\end{equation}

Чутливість, або повнота (англ. sensitivity, recall) - вимірює частку істинно-позитивних зразків серед усіх позитивних зразків.


\begin{equation}
Recall = \frac{TP}{TP+FN}
\end{equation}


Специфічність (англ. specificity) - вимірює частку істинно-негативних зразків серед усіх негативних зразків. 

\begin{equation}
Specificity = \frac{TN}{TN+FP}
\end{equation}

Між влучністю і повнотою, та чутливістю і специфічністю існує обернена залежність, коли можливо підвищити одну ціною зниження іншої. Через наявність такої залежноси, на практиці, ці міри не використовуються окремо. Замість цього використовують агреговані метрики, що дозволяють оцінити якість класифікатора в цілому.

\subsection{Міра F1 та індекс подібності Дайса-Соренсена}

F1 та індекс подібності Дайса-Соренсена (\textit{англ. Dice score}) є еквівалентними метриками, що, зазвичай використовуються в задачах класифікації (F1) та сегментації (Dice score). 

Міра F1 визначається як середнє гармонічне між влучністю та повнотою:

\begin{equation}
F1 = \frac{2}{precision^{-1} + recall^{-1}}=2 \cdot \frac{precision \cdot recall}{precision + recall} = \frac{2TP}{2TP+FP+FN}
\end{equation}
 
Індекс подібності Соренсена визначається на множинах, і, в задачах сегментації, визначається на піксельних масках сегментації:

\begin{equation}
D_c = 2 \cdot \frac{A \cap B}{|A| + |B|}
\end{equation}

Також, для бінарних даних $A \cap B$ відповідає $TP$, а $|A| + |B|$ відповідає всій множині об'єктів, тобто еквівалентно $TP + FP + FN$. Таким чином, коефіцієнт Соренсена є еквівалентним мірі F1. 

\subsection{Каппа Коена}
Оскільки використання точності неможливе в задачах з сильним дисбалансом класів, значення точності можна перенормувати. Перенормування виконується за допомогою статистики chance adjusted index: точність класифікації нормується з використанням точності, яку можна було б отримати випадково. Під випадковою тут розуміється точність рішення, яке отримано з нашого випадковою перестановкою класів:	

\begin{equation}\label{kappa}
\kappa = 1- \frac{\sum_{i=1}^{k} \sum_{j=1}^{k}w_{ij}o_{ij}} {\sum_{i=1}^{k} \sum_{j=1}^{k}w_{ij}e_{ij}},
\end{equation}

Де $k$ - це кількість класів, $o_{ij}$, та $e_{ij}$ це елементи предсказуваних та істинних матриць. $w_{ij}$ розраховується наступним чином для квадратично-зваженої метрики:

\begin{equation}\label{kappa_weights}
w_{ij} = \frac{(i - j)^2}{(k - 1)^2},
\end{equation}

Через чутливість каппи Коена, дослідники повинні ретельно інтерпретувати цю метрику. Наприклад, якщо розглянути дві пари оцінювачів з однаковим відсотком угоди, але різними пропорціями по класах, це суттєво вплине на значення метрики.

Іншою проблемою є кількість класів: із збільшенням кількості класів, метрика підвищується. Крім того, каппа Коена може бути низькою, хоча рівень згоди високий, і окремі оцінки є точними. 

Це робить Каппу Коена мінливим коефіцієнтом для аналізу.

\section{Висновки та постановка задачі дослідження}

В \textbf{першому розділі} наведено огляд літератури за тематикою даної роботи та спорідненими питаннями; висвітлено результати, які були отримані іншими дослідниками. Зокрема, наведено огляд досліджень, що стосуються задачі автоматизованого скринінгу в різних областях; проведено аналіз моделей та методів комп'ютероного зору, насамперед, моделей глибоких нейронних мереж та методів їх навчання. Проведено аналіз методів розпізнавання, класифікації та семантичної сегментації планарних зображень, використання для цього моделей та методів багатозадачного навчання.  Додатково проаналізовано метрики, що використовуються для оцінки достовірності в задачах класифікації та сегментації зображень. Виявлено, що достовірність прогнозів может бути визначена як міра Дайса-Соренсена, або F1-міра. Наведено підходи використання багатозадачного навчання для класифікації та сегментації зображень в задачах автоматизованого скринінгу. 

Проведено аналіз зашумлення розмітки в наборах даних для різних задач автоматизованого скринінгу, який показав, що основними проблемами з розміткою даних в задачах скринінгу є:

\begin{enumerate}
	\item Маски сегментації, що захоплюють сусідні з об'єктом пікселі;
	\item Маски сегментації, що покривають об'єкт не повністю;
	\item Відсутні маски сегментації для деяких об'єктів;
	\item Присутні зайві маски на місцях, де немає об'єктів;
	\item Зображенням присвоєні невірні класи.
\end{enumerate}

Наведено огляд наявних моделей наборів даних, та методів їх генерації. Виявлено, що відсутні моделі наборів даних, які б дозволяли змінювати рівень зашумлення розмітки відповідно до характеристик зашумлення в реальних наборах даних. 

Виявлено, що неузгодженість та незбалансованість навчальних наборів даних для задач автоматизованого скринінгу виникає через відносно малу кількість аномальних прикладів в популяціях та значний рівень похибки розмітників. Це створює проблему при навчанні глибоких нейронних мереж стандартними методами. Тому при побудові моделей класифікації та сегментації потрібно вирішувати задачу \textit{навчання з урахуванням частково-помилкової розмітки}.

В задачах автоматизованого скринінгу важливо зменшення кількості хибно-позитивних результатів діагностики, через відносно малу розповсюдженість аномальних прикладів в популяціях та пов'язані з хибно-позитивними результатами подальші дії. Тому, при побудові систем класифікації та сегментації потрібно зосередитися на \textit{зменшенні хибно-позитивних результатів}. 

Таким чином, проведений огляд сучасної літератури на тему дисертації дозволяє аргументувати актуальність та практичну цінність проведених у роботі досліджень. 

У зв'язку з перерахованим вище, сформульовано мету, задачі об'єкт і предмет дослідження.

\textbf{Метою дослідження} є  підвищення достовірності класифікації та семантичної сегментації планарних зображень в системах автоматизованого скринінгу через розробку моделі та методів аналізу планарних зображень на основі штучних нейронних мереж. 

Для досягнення мети дослідження поставлено і розв'язано такі \textbf{задачі}:

\begin{itemize}
	% раздел 1
	\item Здійснено аналіз зашумлення розмітки в наборах даних для задач автоматизованого скринінгу, проведено аналіз існуючих моделей, методів, алгоритмів та засобів аналізу планарних зображень в системах автоматизованого скринінгу і обґрунтовано вибір напрямку досліджень.
	
	% раздел 2
	\item На основі проведеного аналізу розроблено параметричну модель набору даних, що складається з планарних зображень із зашумленою розміткою для задач класифікації та семантичної сегментації, а також метод його генерації. 
	
	% раздел 3
	\item Розроблено модель нейронної мережі та методи навчання й прогнозування для аналізу планарних зображень, що за рахунок використання декількох семантично-близьких задачах одночасно дозволяють підвищити точність класифікації та семантичної сегментації в умовах зашумлення розмітки даних.
	
	% раздел 4
	\item Розроблено інструментальні засоби, що виконують розроблені моделі та методи, і провести їх випробування в рамках експерименту
	
\end{itemize}

\textbf{\textit{Об'єктом дослідження}} є процес аналізу планарних зображень.

\textbf{\textit{Предмет дослідження}} -- методи класифікації та семантичної сегментації планарних зображень, структури та моделі згорткових нейронних мереж та методи їх навчання.


