\section*{Додаток Д. Вихідні коди програмних інструментальних засобів}

\begin{verbatim}
@registry.Criterion
class NoLoss(nn.Module):
def __init__(self):
super(NoLoss, self).__init__()

def forward(self, inputs, targets):
return inputs
from .model import Net, OneClassNet, MaskOneClassNet
import math
from typing import Optional

import timm
import torch
from efficientnet_pytorch import EfficientNet
from torch import nn
from torch.nn import functional as F
from torch.nn.parameter import Parameter


class Flatten(nn.Module):
def forward(self, input):
return input.view(input.size(0), -1)


class GlobalStdPool2d(nn.Module):
def __init__(self, flatten=False):
super(GlobalStdPool2d, self).__init__()
self.flatten = flatten

def forward(self, x):
if self.flatten:
in_size = x.size()
return x.view((in_size[0], in_size[1], -1)).std(dim=2)
else:
return x.view(x.size(0), x.size(1), -1).std(-1).view(x.size(0), x.size(1), 1, 1)


class AdaptiveConcatPool2d(nn.Module):
def __init__(self, sz=None):
super().__init__()
sz = sz or (1, 1)
self.ap = nn.AdaptiveAvgPool2d(sz)
self.mp = nn.AdaptiveMaxPool2d(sz)

def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)


class MeanStdConcatPool2d(nn.Module):
def __init__(self):
super().__init__()
sz = (1, 1)
self.ap = nn.AdaptiveAvgPool2d(sz)
self.std = GlobalStdPool2d(flatten=False)

def forward(self, x):
return torch.cat([self.std(x), self.ap(x)], 1)


class ArcMarginProduct(nn.Module):
r"""Implement of large margin arc distance: :
Args:
in_features: size of each input sample
out_features: size of each output sample
s: norm of input feature
m: margin
cos(theta + m)
"""

def __init__(self, in_features, out_features):
super(ArcMarginProduct, self).__init__()
self.weight = Parameter(torch.FloatTensor(out_features, in_features))
# nn.init.xavier_uniform_(self.weight)
self.reset_parameters()

def reset_parameters(self):
stdv = 1. / math.sqrt(self.weight.size(1))
self.weight.data.uniform_(-stdv, stdv)

def forward(self, features):
cosine = F.linear(F.normalize(features), F.normalize(self.weight.cuda()))
return cosine


class FeatureExtractorHead(nn.Module):
def __init__(self, extractor, classifier):
super().__init__()

assert extractor is not None
assert classifier is not None

self.fe = extractor
self.cls = classifier

def forward(self, x):
features = self.fe(x)
output = self.cls(features)
return features, output


def freeze(model):
for param in model.parameters():
param.requires_grad = False


def unfreeze(model):
for param in model.parameters():
param.requires_grad = True


def get_classifier_head(input_dim, output_dim, name="concat"):
print('Using classification head ', name)
fe = None
cls = None
if name == "autoencoder":
pass
if name == "avg_arcface":
fe = nn.Sequential(
nn.AdaptiveAvgPool2d((1, 1)),
Flatten(),

nn.BatchNorm1d(input_dim),
nn.Linear(input_dim, 512),
nn.LeakyReLU(),

nn.BatchNorm1d(512),
nn.Dropout(p=0.5),

nn.Linear(512, 512),
nn.BatchNorm1d(512)
)
cls = ArcMarginProduct(512, output_dim)

if name == "arcface":
fe = nn.Sequential(
AdaptiveConcatPool2d((1, 1)),
Flatten(),

nn.BatchNorm1d(input_dim),
nn.Dropout(p=0.25),

nn.Linear(input_dim, 512),
nn.LeakyReLU(),
nn.BatchNorm1d(512),
nn.Dropout(p=0.5),

nn.Linear(512, 512),
nn.BatchNorm1d(512)
)
cls = ArcMarginProduct(512, output_dim)

if name == "std":
fe = nn.Sequential(
GlobalStdPool2d(flatten=True),

nn.BatchNorm1d(input_dim),
nn.Linear(input_dim, 512),
nn.LeakyReLU(),

nn.BatchNorm1d(512),
nn.Dropout(0.5),
nn.Linear(512, 512),
nn.LeakyReLU()
)
cls = nn.Linear(512, output_dim)

if name == "meanstd":
fe = nn.Sequential(
MeanStdConcatPool2d(),
Flatten(),

nn.BatchNorm1d(input_dim),
nn.Linear(input_dim, 512),
nn.LeakyReLU(),

nn.BatchNorm1d(512),
nn.Dropout(0.5),
nn.Linear(512, 512),
nn.LeakyReLU()
)
cls = nn.Linear(512, output_dim)

if name == "concat2":
fe = nn.Sequential(
AdaptiveConcatPool2d((1, 1)),
Flatten(),

nn.BatchNorm1d(input_dim),
nn.Linear(input_dim, 512),
nn.LeakyReLU(),

nn.BatchNorm1d(512),
nn.Linear(512, 512),
nn.LeakyReLU(),

nn.BatchNorm1d(512),
nn.Dropout(p=0.5)
)
cls = nn.Linear(512, output_dim)

if name == "concat":
fe = nn.Sequential(
AdaptiveConcatPool2d((1, 1)),
Flatten(),
nn.BatchNorm1d(input_dim),
nn.Linear(input_dim, 256),
nn.LeakyReLU(),
nn.Dropout(0.5)
)
cls = nn.Linear(256, output_dim)

if name == "avg2":
fe = nn.Sequential(
nn.AdaptiveAvgPool2d((1, 1)),
Flatten(),

nn.BatchNorm1d(input_dim),
nn.Linear(input_dim, 512),
nn.LeakyReLU(),

nn.BatchNorm1d(512),
nn.Dropout(0.5),
nn.Linear(512, 512),
nn.LeakyReLU()
)
cls = nn.Linear(512, output_dim)

if name == "avg":
fe = nn.Sequential(
nn.AdaptiveAvgPool2d((1, 1)),
Flatten(),
nn.BatchNorm1d(input_dim),
nn.Dropout(0.5),
nn.Linear(input_dim, 256),
nn.LeakyReLU()
)
cls = nn.Linear(256, output_dim)

if name == "max":
fe = nn.Sequential(
nn.AdaptiveMaxPool2d((1, 1)),
Flatten(),
nn.BatchNorm1d(input_dim),
nn.Linear(input_dim, 256),
nn.LeakyReLU(),
nn.Dropout(0.5)
)
cls = nn.Linear(256, output_dim)

if name == "simple_avg":
fe = nn.Sequential(
nn.AdaptiveAvgPool2d((1, 1)),
Flatten(),
nn.BatchNorm1d(input_dim)
)
cls = nn.Linear(input_dim, output_dim)

if name == "one_layer_fc":
fe = nn.Sequential(
nn.AdaptiveAvgPool2d((1, 1)),
Flatten(),
)
cls = nn.Linear(input_dim, output_dim)

return FeatureExtractorHead(extractor=fe, classifier=cls)


class Net(nn.Module):
"""
efficientnet-b0-224
efficientnet-b1-240
efficientnet-b2-260
efficientnet-b3-300
efficientnet-b4-380
efficientnet-b5-456
efficientnet-b6-528
efficientnet-b7-600
"""

def __init__(
self,
num_classes: int,
n_features: int = 2048,
arch: str = "se_resnext50_32x4d",
class_head_name: str = "one_layer_fc",
init_bias: Optional[float] = None):
super().__init__()

self.arch = arch
self.n_features = n_features
self.init_bias = init_bias

if "lukemelas" in arch:
arch = arch.split("_")[1]
self.body = EfficientNet.from_pretrained(arch)
else:
self.body = timm.create_model(arch, pretrained=True)

self.classification_head = get_classifier_head(n_features, num_classes, name=class_head_name)

self.fill_bias()

def fill_bias(self):
bias = self.init_bias
if bias is not None and hasattr(self.classification_head.cls, "bias"):
print(f"Initializing bias with {bias}")
nn.init.constant_(self.classification_head.cls.bias, bias)

def forward(self, x):
if "lukemelas" in self.arch:
x = self.body.extract_features(x)
else:
x = self.body.forward_features(x)

features, classification_ohe = self.classification_head(x)
classification_ohe_softmax = F.softmax(classification_ohe, dim=1)
classification_logits = torch.argmax(classification_ohe_softmax, axis=1) * 1.0

ret = {
	'features': features,
	'logit_ohe': torch.squeeze(classification_ohe),
	'logit_ohe_softmax': torch.squeeze(classification_ohe_softmax),
	'logit': classification_logits,
	'logit_prob_positive': 1 - torch.squeeze(classification_ohe_softmax)[:, 0]
}

return ret


class OneClassNet(Net):
def __init__(self, num_classes: int = 1,
n_features: int = 2048,
arch: str = "se_resnext50_32x4d",
class_head_name: str = "one_layer_fc",
init_bias: Optional[float] = None
):

assert num_classes == 1

super().__init__(num_classes=num_classes,
n_features=n_features,
arch=arch,
class_head_name=class_head_name,
init_bias=init_bias)

def forward(self, x):
if "lukemelas" in self.arch:
x = self.body.extract_features(x)
else:
x = self.body.forward_features(x)

features, logits = self.classification_head(x)
ret = {
	'backbone_features': x,
	'features': features,
	'logits': logits,
}

return ret

def predict(self, x):
res = self.forward(x)

return res["features"], res["backbone_features"], res["logits"]


class MaskOneClassNet(Net):
def __init__(self, num_classes: int = 1,
n_features: int = 2048,
arch: str = "se_resnext50_32x4d",
class_head_name: str = "one_layer_fc",
init_bias: Optional[float] = None
):

assert num_classes == 1

super().__init__(num_classes=num_classes,
n_features=n_features,
arch=arch,
class_head_name=class_head_name,
init_bias=init_bias)

self.mask_branch = nn.Sequential(
nn.Dropout2d(0.5),

nn.Conv2d(n_features, 256, kernel_size=1),
nn.BatchNorm2d(256),
nn.ReLU(inplace=True),

nn.Conv2d(256, 256, kernel_size=1),
nn.BatchNorm2d(256),
nn.ReLU(inplace=True),

nn.Conv2d(256, 1, kernel_size=1),
)

def forward(self, x):
if "lukemelas" in self.arch:
x = self.body.extract_features(x)
else:
x = self.body.forward_features(x)

features, logits = self.classification_head(x)
mask = self.mask_branch(x).sigmoid()

melanoma = mask * logits.view(-1, 1, 1, 1)
melanoma = melanoma.sum(dim=(2, 3)) / (mask.sum(dim=(2, 3)) + 1e-4)

ret = {
	'backbone_features': x,
	
	'features': features,
	'logits': melanoma,
	'mask': mask
}

return ret

def predict(self, x):
res = self.forward(x)

return res["features"], res["backbone_features"], res["logits"]
import math
import torch
from torch.optim.optimizer import Optimizer, required

from catalyst.dl import registry

@registry.Optimizer
class RAdamMy(Optimizer):
def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-4, weight_decay=0):
if not 0.0 <= lr:
raise ValueError("Invalid learning rate: {}".format(lr))
if not 0.0 <= eps:
raise ValueError("Invalid epsilon value: {}".format(eps))
if not 0.0 <= betas[0] < 1.0:
raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
if not 0.0 <= betas[1] < 1.0:
raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))

defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
self.buffer = [[None, None, None] for ind in range(10)]
super(RAdamMy, self).__init__(params, defaults)

def __setstate__(self, state):
super(RAdamMy, self).__setstate__(state)

def step(self, closure=None):

loss = None
if closure is not None:
loss = closure()

for group in self.param_groups:

for p in group["params"]:
if p.grad is None:
continue
grad = p.grad.data.float()
if grad.is_sparse:
raise RuntimeError("RAdam does not support sparse gradients")

p_data_fp32 = p.data.float()

state = self.state[p]

if len(state) == 0:
state["step"] = 0
state["exp_avg"] = torch.zeros_like(p_data_fp32)
state["exp_avg_sq"] = torch.zeros_like(p_data_fp32)
else:
state["exp_avg"] = state["exp_avg"].type_as(p_data_fp32)
state["exp_avg_sq"] = state["exp_avg_sq"].type_as(p_data_fp32)

exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
beta1, beta2 = group["betas"]

exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
exp_avg.mul_(beta1).add_(1 - beta1, grad)

state["step"] += 1
buffered = self.buffer[int(state["step"] % 10)]
if state["step"] == buffered[0]:
N_sma, step_size = buffered[1], buffered[2]
else:
buffered[0] = state["step"]
beta2_t = beta2 ** state["step"]
N_sma_max = 2 / (1 - beta2) - 1
N_sma = N_sma_max - 2 * state["step"] * beta2_t / (1 - beta2_t)
buffered[1] = N_sma

# more conservative since it's an approximated value
if N_sma >= 5:
step_size = math.sqrt(
(1 - beta2_t)
* (N_sma - 4)
/ (N_sma_max - 4)
* (N_sma - 2)
/ N_sma
* N_sma_max
/ (N_sma_max - 2)
) / (1 - beta1 ** state["step"])
else:
step_size = 1.0 / (1 - beta1 ** state["step"])
buffered[2] = step_size

if group["weight_decay"] != 0:
p_data_fp32.add_(-group["weight_decay"] * group["lr"], p_data_fp32)

# more conservative since it's an approximated value
if N_sma >= 5:
denom = exp_avg_sq.sqrt().add_(group["eps"])
p_data_fp32.addcdiv_(-step_size * group["lr"], exp_avg, denom)
else:
p_data_fp32.add_(-step_size * group["lr"], exp_avg)

p.data.copy_(p_data_fp32)

return loss


class PlainRAdam(Optimizer):
def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):
if not 0.0 <= lr:
raise ValueError("Invalid learning rate: {}".format(lr))
if not 0.0 <= eps:
raise ValueError("Invalid epsilon value: {}".format(eps))
if not 0.0 <= betas[0] < 1.0:
raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
if not 0.0 <= betas[1] < 1.0:
raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))

defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)

super(PlainRAdam, self).__init__(params, defaults)

def __setstate__(self, state):
super(PlainRAdam, self).__setstate__(state)

def step(self, closure=None):

loss = None
if closure is not None:
loss = closure()

for group in self.param_groups:

for p in group["params"]:
if p.grad is None:
continue
grad = p.grad.data.float()
if grad.is_sparse:
raise RuntimeError("RAdam does not support sparse gradients")

p_data_fp32 = p.data.float()

state = self.state[p]

if len(state) == 0:
state["step"] = 0
state["exp_avg"] = torch.zeros_like(p_data_fp32)
state["exp_avg_sq"] = torch.zeros_like(p_data_fp32)
else:
state["exp_avg"] = state["exp_avg"].type_as(p_data_fp32)
state["exp_avg_sq"] = state["exp_avg_sq"].type_as(p_data_fp32)

exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
beta1, beta2 = group["betas"]

exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
exp_avg.mul_(beta1).add_(1 - beta1, grad)

state["step"] += 1
beta2_t = beta2 ** state["step"]
N_sma_max = 2 / (1 - beta2) - 1
N_sma = N_sma_max - 2 * state["step"] * beta2_t / (1 - beta2_t)

if group["weight_decay"] != 0:
p_data_fp32.add_(-group["weight_decay"] * group["lr"], p_data_fp32)

# more conservative since it's an approximated value
if N_sma >= 5:
step_size = (
group["lr"]
* math.sqrt(
(1 - beta2_t)
* (N_sma - 4)
/ (N_sma_max - 4)
* (N_sma - 2)
/ N_sma
* N_sma_max
/ (N_sma_max - 2)
)
/ (1 - beta1 ** state["step"])
)
denom = exp_avg_sq.sqrt().add_(group["eps"])
p_data_fp32.addcdiv_(-step_size, exp_avg, denom)
else:
step_size = group["lr"] / (1 - beta1 ** state["step"])
p_data_fp32.add_(-step_size, exp_avg)

p.data.copy_(p_data_fp32)

return loss


class AdamW(Optimizer):
def __init__(
self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, warmup=0
):
if not 0.0 <= lr:
raise ValueError("Invalid learning rate: {}".format(lr))
if not 0.0 <= eps:
raise ValueError("Invalid epsilon value: {}".format(eps))
if not 0.0 <= betas[0] < 1.0:
raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
if not 0.0 <= betas[1] < 1.0:
raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))

defaults = dict(
lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, warmup=warmup
)
super(AdamW, self).__init__(params, defaults)

def __setstate__(self, state):
super(AdamW, self).__setstate__(state)

def step(self, closure=None):
loss = None
if closure is not None:
loss = closure()

for group in self.param_groups:

for p in group["params"]:
if p.grad is None:
continue
grad = p.grad.data.float()
if grad.is_sparse:
raise RuntimeError(
"Adam does not support sparse gradients, please consider SparseAdam instead"
)

p_data_fp32 = p.data.float()

state = self.state[p]

if len(state) == 0:
state["step"] = 0
state["exp_avg"] = torch.zeros_like(p_data_fp32)
state["exp_avg_sq"] = torch.zeros_like(p_data_fp32)
else:
state["exp_avg"] = state["exp_avg"].type_as(p_data_fp32)
state["exp_avg_sq"] = state["exp_avg_sq"].type_as(p_data_fp32)

exp_avg, exp_avg_sq = state["exp_avg"], state["exp_avg_sq"]
beta1, beta2 = group["betas"]

state["step"] += 1

exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
exp_avg.mul_(beta1).add_(1 - beta1, grad)

denom = exp_avg_sq.sqrt().add_(group["eps"])
bias_correction1 = 1 - beta1 ** state["step"]
bias_correction2 = 1 - beta2 ** state["step"]

if group["warmup"] > state["step"]:
scheduled_lr = 1e-8 + state["step"] * group["lr"] / group["warmup"]
else:
scheduled_lr = group["lr"]

step_size = (
scheduled_lr * math.sqrt(bias_correction2) / bias_correction1
)

if group["weight_decay"] != 0:
p_data_fp32.add_(-group["weight_decay"] * scheduled_lr, p_data_fp32)

p_data_fp32.addcdiv_(-step_size, exp_avg, denom)

p.data.copy_(p_data_fp32)

return lossfrom .warmup import GradualWarmupScheduler
from torch.optim.lr_scheduler import _LRScheduler
from torch.optim.lr_scheduler import ReduceLROnPlateau


class GradualWarmupScheduler(_LRScheduler):
""" Gradually warm-up(increasing) learning rate in optimizer.
Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.

Args:
optimizer (Optimizer): Wrapped optimizer.
multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.
total_epoch: target learning rate is reached at total_epoch, gradually
after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)
"""

def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):
self.multiplier = multiplier
if self.multiplier < 1.:
raise ValueError('multiplier should be greater thant or equal to 1.')
self.total_epoch = total_epoch
self.after_scheduler = after_scheduler
self.finished = False
super(GradualWarmupScheduler, self).__init__(optimizer)

def get_lr(self):
if self.last_epoch > self.total_epoch:
if self.after_scheduler:
if not self.finished:
self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]
self.finished = True
return self.after_scheduler.get_last_lr()
return [base_lr * self.multiplier for base_lr in self.base_lrs]

if self.multiplier == 1.0:
return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]
else:
return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in
self.base_lrs]

def step_ReduceLROnPlateau(self, metrics, epoch=None):
if epoch is None:
epoch = self.last_epoch + 1
self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning
if self.last_epoch <= self.total_epoch:
warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in
self.base_lrs]
for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):
param_group['lr'] = lr
else:
if epoch is None:
self.after_scheduler.step(metrics, None)
else:
self.after_scheduler.step(metrics, epoch - self.total_epoch)

def step(self, epoch=None, metrics=None):
if type(self.after_scheduler) != ReduceLROnPlateau:
if self.finished and self.after_scheduler:
if epoch is None:
self.after_scheduler.step(None)
else:
self.after_scheduler.step(epoch - self.total_epoch)
self._last_lr = self.after_scheduler.get_last_lr()
else:
return super(GradualWarmupScheduler, self).step(epoch)
else:
self.step_ReduceLROnPlateau(metrics, epoch)
\end{verbatim}