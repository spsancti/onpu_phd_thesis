\chapter{Нейромережева модель аналізу планарних зображень та методи її навчання}

Запропоновано модель багатозадачної нейронної мережі, що дозволяє вирішувати задачі семантичної сегментації та класифікації для аналізу планарних зображень, а також метод її навчання, що заснований на методах багатозадачного навчання. 

На відміну від однозадачного машинного навчання, багатозадачне машинне навчання дозволяє використати спільності та відмінності між різними задачами для підвищення узагальнювальної здатності на кожній з окремих задач. Використання комбінації декількох задач машинного навчання може сприяти вивченню кращих наборів ознак, та підвищити точність роботи нейронних мереж. Також, сумісне використання прогнозів декількох задач для уточнення кожного з окремих прогнозів, так і побудови нових. 

Основним припущенням, на яке спирається розроблений метод є можливість створення більш точної розмітки тренувального набору даних для задачі класифікації на основі розмітки семантичної сегментації для того ж набору вхідних зображень. В даному контексті, задача семантичної сегментації називається основною задачею, а класифікації - додатковою.


\section{Модель багатозадачної штучної нейронної мережі}
\label{nn-arch}

Для того, щоб основна і додаткова задачі оновлювали параметри нейронної мережі, що відповідають за виділення ознак, використано схему багатозадачного навчання з жорстким розподілом параметрів. 

\subsection{Загальна структура нейронної мережі}

Нейронна мережа виконана з використанням архітектури енкодер-декодер, заснована на архітектурі нейронних мереж LinkNet \cite{linknet}. На відміну від оригінальної структури LinkNet, запропонована модель нейронної мережі складається з одного енкодера та двох декодерів: для задач сегментації та класифікації відповідно.  

В ролі енкодера можуть бути використані наявні багатостадійні архітектури, такі як VGGNet, ResNet, EfficientNet та ін. Карти ознак після кожного етапу просторового зменшення використовуються як входи для декодера сегментації, для декодера класифікації використовується карта ознак з найглибшого шару енкодера. Узагальнену архітектуру зображено на рисунку \ref{fig:my_net_arch} 

\begin{figure}
	\centering
	\includegraphics[width=16cm]{my_ne_arch.png}
	\caption{Загальна архітектура нейронної мережі з декодером та класифікатором}
	\label{fig:my_net_arch}
\end{figure} 


Таким чином, виконується жорсткий розподіл параметрів, оскільки параметри енкодера відповідають одночасно за обидві задачі. Така структура дозволяє використати методи трансферного навчання (індуктивного переносу) для підвищення роздільності внутрішніх представлень і пришвидшення процесу навчання: набір параметрів енкодера $\theta_{enc}$ ініціалізується з використанням параметрів, отриманих після навчання енкодера на наборі даних Imagenet \cite{imagenet}. Декодери ініціалізується з використанням методу Хе: $\theta \in \mathcal{U}(-b, b)$, де $b$ - константа залежна від типу шару \cite{kaiming_uniform}


Окрім графічного представлення, запропонована модель може бути представлена алгебраїчно: нехай нейронну мережу енкодера визначено як $F_{encoder}$. Тоді для вхідного зображення $x \in \mathcal{R} ^ {3 \times H \times W}$,  $v_1, v_2, ... v_i, ... v_n$ - набір карт ознак, так що $v_i \in \mathcal{R} ^ {c_i \times \frac{H}{i} \times \frac{W}{i}}$, де $c_i$ - кількість каналів для кожної з карт ознак, а $n$ - кількість стадій енкодера (залежить від архітектури):

\begin{equation}
	\label{eqn:enc_features}
	v_1, v_2 ... v_n = F_{encoder}(x, \theta_{enc})
\end{equation}

де $\theta_{enc}$ - набір параметрів енкодера.

Тоді, нейронні мережі декодера сегментації та класифікації можна визначити як $F_{seg}$ та $F_{cls}$ відповідно. Для набору карт ознак, що генерує енкодер, маємо:

\begin{align}
	\label{eqn:m_seg}
	M_{seg} &= F_{seg}((v_1, v_2 ... v_n), \theta_{seg}) \\
	\label{eqn:c_cls}
	C_{cls} &= F_{cls}((v_n), \theta_{cls})
\end{align}

де $\theta_{seg}$ та $\theta_{cls}$ - набори параметрів декодерів сегментації та класифікації відповідно.

Окрім загальної архітектури моделі, критичними для коректної роботи запропонованих методів є структури декодерів для кожної з задач, зокрема, нормалізація їх внутрішніх представлень.

\subsection{Структура декодера семантичної сегментації}

Декодер складається з декількох стадій, кожна з яких має розміри карт ознак відповідно до карт ознак енкодера. 

Структура декодера сегментації повторює декодер архітектури UNet \cite{unet}. На відміну від архітектури UNet, в даному декодері застосовується пакетна нормалізація після операцій конкатенації з шарами енкодера та перед згортковими шарами.  Ця нормалізація необхідна для забезпечення незалежності масштабів значень карт ознак на різних шарах декодера сегментації та енкодера, які також спільно використовуються декодером класифікації. 

Також, замість зворотних згорток, в декодері використовується білінійна інтерполяція для підвищення просторового розширення карт ознак, що дозволяє уникнути "шахового патерну" в прогнозованих масках \cite{upconv_checkerboard}/

Оскільки в задачі сегментації з частково-помилковою розміткою одному пікселю може бути призначено декілька класів одночасно, необхідно це враховувати при використанні функцій втрат. Для формування фінальної карти сегментації використовується спеціальна версія блоку декодера, що має сигмоподібну функцію активації. Це дозволяє нейронній мережі прогнозувати одночасно декілька класів для одного пікселя, замість спроб виділення одного конкретного класу при використанні нормованої експоненційної функції (Softmax).

Структуру окремих стадій показано на рисунку \ref{fig:my_decoder_block_arch}.

\begin{figure}
	\centering
	\includegraphics[width=14cm]{my_decoder_block_arch.png}
	\caption{Структура стадій декодеру сегментації}
	\label{fig:my_decoder_block_arch}
\end{figure} 

В ролі вхідної карти ознак для найпершого шару декодера використовується останній шар енкодера $v_n$. 

\subsection{Структура декодера класифікації}

Оскільки, в даному випадку, на зображенні може бути декілька об'єктів, а задача класифікації є задачею навчання за набором зразків, декодер класифікації має враховувати як глобальний просторовий контекст, так і локальні ознаки, притаманні об'єктам. Для цього, першим шаром класифікатора є конкатенація результатів двох операцій глобальної підвибірки: з операцією усереднення (GlobalAvgPooling), та вибору максимуму (GlobalMaxPooling) для кожного з каналів карти ознак енкодера. 

Так само, як і в декодері сегментації, для забезпечення незалежності масштабу останнього шару енкодера, першим шаром є шар пакетної нормалізації.

Для підвищення роздільності ознак останнього шару енкодера, класифікатор являє собою лінійну функцію, що спонукає енкодер до формування лінійно-роздільних представлень $v_n$, які також використовуються декодером сегментації. Ця структура знижує кількість нелінійних перетворень, потрібних декодеру. 

Структура декодера класифікації є важливою складовою успішного навчання при використанні обмеженої функції втрат в задачі класифікації. Основними задачами декодеру класифікації є провадження градієнтів до енкодера у випадку фільтрації неправильної розмітки в задачі сегментації, а також покращення роздільності ознак найглибшого шару енкодера.

Структура декодера класифікації зображена на рисунку \ref{fig:my_cls_arch}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=8cm]{my_cls_arch.png}
	\caption{Структура декодера класифікації}
	\label{fig:my_cls_arch}
\end{figure} 



















% 10 страниц ------------------------------------------------------------------------
\section{Методи навчання та прогнозування багатозадачних ШНМ в умовах частково помилкової розмітки семантичної сегментації}

На основі запропонованої моделі багатозадачної нейронної мережі розроблено метод багатозадачного навчання, що в умовах частково-помилкової розмітки дозволяє зменшити вплив неправильно розмічених прикладів на процес навчання за рахунок їх ефективної фільтрації безпосередньо під час навчання. Так, часто на одному зображенні можуть бути розташовані як коректно-розмічені, так і некоректно-розмічені об'єкти. Для підвищення точності навчання нейронних мереж в таких умовах, запропоновано використати метод багатозадачного навчання з використанням похідної більш загальної задачі.



\subsection{Автоматична фільтрація помилкової розмітки}

Маючи велику кількість параметрів, сучасні нейронні мережі здатні до запам'ятовування тренувального набору даних замість вивчення корисних ознак для узагальнення на інші набори даних (перенавчання). В умовах наявності помилкової розмітки в тренувальному наборі даних, нейронні мережі схильні до вивчення і помилок розмітки на протязі навчання. 

Традиційні методи запобігання перенавчанню, такі як аугментація даних, регуляризація ваг мережі, та завчасна зупинка навчання хоча і показують покращені результати, але можуть виявитися недостатніми в умовах великої ймовірності помилок в навчальних даних. Також, вони можуть погіршити якість навчання через зменшення ємності нейронної мережі, або, при надмірному використанні, можуть спонукати нейронну мережу акцентувати увагу на текстурах, чо часто є недопустимим в задачах семантичної сегментації. 

Для того, щоб ефективно зменшити вплив помилкової розмітки, необхідно вилучити її з процесу навчання нейронної мережі. Якщо неможливо вилучити неправильно розмічені приклади з набору даних до початку навчання, це можна робити ітеративно в процесі навчання. Такі функції втрат, як перехресна ентропія, або фокальна функція втрат призначають експоненційно високе значення для неправильних прогнозів нейронних мереж, що має шкідливий ефект при наявності помилкової розмітки в тренувальному наборі даних - правильні прогнози на неправильно-розмічених даних створюють конфліктні градієнти та знижують впевненість прогнозів нейронної мережі.

Для виключення неправильної розмітки в процесі навчання запропоновано обмеження функції втрат, з відповідним визначенням градієнтів для нової функції:

\begin{equation}
	\mathcal{L} \rceil = min(L, \theta)
\end{equation}

де $\theta$ - поріг обмеження функції втрат.

Графіки обмежених зверху функцій втрат зображено на рисунку \ref{fig:trimmed_losses}.
\ref{fig:trimmed_losses}
\begin{figure}
	\centering
	\includegraphics[width=12cm]{trimmed_losses_en.png}
	\caption{Приклади обмежених зверху функцій втрат}
	\label{fig:trimmed_losses}
\end{figure} 

Для функції обмеження зверху градієнт визначено лише на проміжку $(-\inf, \theta]$, тому для проміжку $(\theta, \inf)$ градієнт встановлено рівним нулю:

\begin{equation}
	\mathcal{\nabla} min(L, \theta) = 
	\begin{cases}
		1 &\text{$L \in (-\inf, \theta]$}\\
		0 &\text{$L \in (\theta, \inf)$}
	\end{cases}
\end{equation}

Всі загальновживані методи оптимізації параметрів нейронних мереж засновані на методі оновлення параметрів першого порядку, що, в загальному випадку, є модифікацією стохастичного градієнтного спуску: 

\begin{equation}
	\theta \leftarrow \theta - \eta \nabla_\theta L
\end{equation}

Де $\theta$ - параметри нейронної мережі, $\eta$ - темп навчання, а $\nabla_\theta$ - градієнт функції втрат відносно параметрів, таким чином, приклади з занадто великою похибкою (переважно з помилковою розміткою) ефективно виключаються з навчання, оскільки мають нульовий градієнт відносно параметрів.

Недоліком цього методу фільтрації помилкової розмітки є можливість виключення прикладів, що занадто складні для вивчення нейронною мережею на ранніх етапах навчання. Це може сповільнити процес навчання, або взагалі зупинити його. Щоб запобігти сповільненню процесу, запропоновано використання багатозадачного навчання з використанням додаткової більш загальної задачі. 






\subsection{Генерація похідної задачі до задачі семантичної сегментації}

Для того, щоб забезпечити успішне навчання нейронних мереж на ранніх етапах, та врахувати занадто складні приклади, необхідно ввести інформацію про ці об'єкти в обхід відфільтрованої розмітки. 

На основі розмітки для задачі семантичної сегментації може бути побудовано розмітку для інших більш загальних задач, таких як задача детекції (наприклад, обмежувальні прямокутники навколо об'єктів), регресії (наприклад, координати центрів об'єктів), або класифікації (класи об'єктів). Такі задачі є більш зашальними, оскільки мають менше незалежних змінних в виходах нейронної мережі. Наприклад, замість класифікації кожного з пікселів зображення в задачі семантичної сегментації, в задачі детекції потрібно лише прогнозування класів та координат обмежувальних прямокутників.  

Також, з точки зору багатозадачного машинного навчання, ці задачі є семантично-близькими: такі задачі, що мають однакові вхідні дані, та пов'язані вихідні дані \cite{caruana}.

На відміну від \cite{arxiv:1412.0069}, що спирається на вивчення більш детальних семантично-близьких задач, в розробленому методі використання більш точних даних, для загальніших задач дозволяє покращити роздільність внутрішніх представлень нейронної мережі, що, в свою чергу, покращує результати на вихідній задачі. Також, оскільки задачі є семантично близькими, не відбувається конфлікту градієнтів, що є типовим при навчанні семантично-різнорідних задач \cite{gradient_conflict}.

Оскільки задача семантичної сегментації може бути розглянута як задача піксельної класифікації, можна розглядати задачу класифікації всього зображення як більш загальну до неї. В такому контексті, класифікація буде окремим випадком навчання за набором зразків \cite{multiinstance}: замість розмітки кожного з об'єктів для всіх класів на зображенні, зображення являє собою мішок з одним, чи декількома об'єктами та відповідним маркуванням, чи є об'єкти заданих класів на ньому.

Попередній аналіз показав, що неточна розмітка в задачах сегментації полягає в наявності зайвих, або у відсутності деяких розмічених пікселів, або об'єктів. Наявність такої неточної розмітки дозволяє створити на її основі точну розмітку для класифікації.

Нехай для зображення $x \in \mathcal{R}^{3 \times H \times W}$ існує маска сегментації $y_s \in \mathcal{R}^{C \times H \times W}$ де $C$ - кількість класів, а $H$ та $W$ висота та ширина зображення відповідно. Якщо в масці сегментації є хоча б один розмічений об'єкт класу $с$, встановлюється мітка відповідного класу $y_c \in (0, 1)^C$ в розмітці задачі класифікації:

\begin{equation}
	y_c = t < \sum_{i}^{H} \sum_{j}^{W} y_{s_{ij}}
\end{equation}

де $t$ - поріг мінімального розміру об'єкта в пікселях

Згенерована таким чином задача класифікації має меншу ймовірність хибної розмітки. 

В такому випадку, обмеження функції втрат використовується лише для оригінальної задачі, а більш загальна додаткова задача використовує оригінальну функцію втрат без обмеження. Таким чином, завжди існують градієнти від більш загальної розмітки, що спонукають навчання на прикладах, для яких немає градієнтів через обмеження функції втрат.





 
\subsection{Агрегація функцій втрат під час навчання}

Оскільки модель багатозадачної нейронної мережі має два виходи для кожної з задач, для одночасного оновлення всіх параметрів необхідне одночасне використання двох функцій втрат. Для кожної з задач окремо обчислюється функція втрат. Для навчання декодера сегментації використовується обмежена зверху, в той час як для декодера класифікації - звичайна. Кожна з функцій втрат нормалізується відносно кількості прикладів в одному пакеті навчання для зниження впливу цього гіперпараметру. 

Загальне значення функції втрат визначається як арифметичне середнє між індивідуальними значеннями:

\begin{equation}
	L_{total} = \frac{L_{seg} \rceil + L_{cls}}{2}
\end{equation}
 
Відповідно, загальний градієнт функції втрат буде сумою градієнтів складових частин:

\begin{equation}
	\nabla L_{total} = \frac{\nabla L_{seg} \rceil + \nabla  L_{cls}}{2}
\end{equation}


Під час навчання нейронної мережі використовується стандартний алгоритм зворотного поширення помилки з оптимізатором Adam \cite{adam-optimizer}, що відрізняється від методу стохастичного градієнтного спуску наявністю адаптивної нормалізації моментів градієнтів. 


Поріг обмеження функції втрат для задачі сегментації є параметром алгоритму навчання та має обиратися емпірично в залежності від рівня помилок в розмітці. Таким чином, забезпечується "прохід" градієнтів для оновлення параметрів від хоча б однієї функції втрат для кожного вхідного прикладу. 









% 10 страниц ------------------------------------------------------------------------
\subsection{Багатозадачне прогнозування результатів семантичної сегментації}


Оскільки запропонована модель нейронної мережі може одночасно вивчати як задачу сегментації, так і задачу класифікації, стає можливою імплементація фільтрування хибно-позитивних ознак без підвищення витрат часу на етапі прогнозування результатів. 

Для фільтрації хибно-позитивних результатів запропоновано метод комбінації задач класифікації та сегментації, що дозволяє використовувати задачу класифікації, що попередньо була навчена на менш зашумленій розмітці як фільтр для задачі семантичної сегментації. В такому випадку можливі два варіанти комбінації двох задач: послідовний і паралельний.

Нехай $C_{cls} \in \mathcal{R}^{C}$ та $M_{seg} \in \mathcal{R}^{C \times H \times W}$- результати декодерів класифікації та сегментації відповідно, значення яких знаходяться на проміжку  $(- \infty, + \infty)$ (логіти).

Для отримання результатів на проміжку $[0, 1]$ використовується логістична сигмоїдна функція активації:

\begin{equation}
	\sigma(x) = \frac{1}{1 + e^{-x}}
\end{equation}

Послідовний варіант полягає у відкиданні результатів сегментації, якщо результат класифікації менший, за деякий поріг $t_{cls}$:

\begin{equation}
M_{refined} = 	
\begin{cases}
	\sigma(M_{seg})& \text{якщо } \sigma(C_{cls}) > t_{cls}\\
	0^{C \times H \times W} &\text{якщо } \sigma(C_{cls}) \leq t_{cls}
\end{cases}
\end{equation}

Такий варіант має декілька суттєвих недоліків:
\begin{itemize}
	\item Необхідність вибору додаткового параметра $\theta_{cls}$ вносить додаткову можливість помилки; неправильний підбір цього параметра може призвести до значного збільшення хибно-негативних результатів.
	\item Можливість розповсюдження помилки класифікатора виникає через значно меншу кількість параметрів порівняно з декодером сегментації.
\end{itemize}

Паралельний варіант полягає у зважуванні карти сегментації за допомогою нормованих логітів класифікатора. Першим кроком є трансформація логітів сегментації та класифікації в некалібровані оцінки на проміжку $[0, 1]$:

\begin{align}
	\label{eqn:m_hat_seg}
	\hat{M}_{seg} &= \sigma(M_{seg}) \\
	\label{eqn:c_hat_cls}
	\hat{C}_{cls} &= \sigma(C_{cls})	
\end{align}

Ці оцінки мають ті самі розмірності, що й оригінальні маска та класи, для зручності репрезентації операцій додано додаткові розмірності до вектору класів: $\hat{M}_{seg} \in \mathcal{R}^{C \times H \times W}$ та $\hat{C}_{cls} \in \mathcal{R}^{C \times 1 \times 1}$ 

Зважування карти сегментації відбувається за допомогою добутку Адамара між матрицями $\hat{M}_{seg}$ та $\hat{C}_{cls}$

\begin{equation}
	M_{refined} = \hat{M}_{seg} \circ \hat{C}_{cls}
\end{equation}

Графічну репрезентацію структури зображено на рисунку \ref{fig:my_net_arch_comb}.

\begin{figure}
	\centering
	\includegraphics[width=16cm]{my_net_arch_comb.png}
	\caption{Структура об'єднання прогнозів класифікації та сегментації}
	\label{fig:my_net_arch_comb}
\end{figure} 

Паралельний варіант має перевагу над послідовним у відсутності додаткового параметру навчання. Також, зважені карти сегментації допомагають збереженню більшої кількості інформації для етапу пост-обробки масок сегментації. 

Однак, в разі використання добутку двох сигмоїдних функцій, потрібно зважати, що змінюється центральна точка, і, відповідно поріг бінаризації в подальшій обробці. Навіть у випадку співпадіння значень класифікації та сегментації, фінальний прогноз буде відрізнятися від послідовного підходу. 

Графік сигмоїдної функції, та добутку двох ідентичних сигмоїдних функцій зображено на рисунку \ref{fig:sig_sig_sqr}.

\begin{figure}
	\centering
	\includegraphics[width=12cm]{sig_sig_sqr.png}
	\caption{Структура декодера класифікації}
	\label{fig:sig_sig_sqr}
\end{figure}

При використанні того самого порогу бінаризації для карт сегментації, зменшиться кількість позитивних пікселів в результаті, тому при прямому переході між однозадачним методом прогнозування, потрібно використовувати квадрат $t_{cls}$:

\begin{equation}
	\hat{t}_{cls} = t_{cls}^2
\end{equation}

Таким чином, кількість позитивних пікселів не заміниться за рахунок зміни в нормалізації сигмоїд.










% 10 страниц ------------------------------------------------------------------------
\section{Метод локалізації важливих для класифікації ознак зображення в умовах відсутності розмітки для сегментації в навчальному наборі даних}

На основі двох представлених методів, розроблено метод навчання з частковим залученням вчителя та метод пост-обробки, що дозволяє виконувати локалізацію важливих для класифікації ознак на вихідному зображенні, що є корисним під час інтерпретації прогнозів моделей. За рахунок використання методів багатозадачного навчання досягається можливість застосування методу в умовах відсутності розмітки семантичної сегментації в навчальному наборі даних. 

Даний метод застосовується для навчання тієї самої моделі, але за умови наявності лише розмітки для задачі класифікації. Так само, як і в розділі \ref{nn-arch} використані наступні позначення:

$x$ - вхідне зображення енкодера розмірами $3 \times H \times W$

$F_{encoder}(x, \theta_{enc})$ - функція енкодера зображень,

$\theta_{enc}$ - набір параметрів нейронної мережі енкодера,

$v_1, v_2, ... v_i, ... v_n$ - набір карт ознак енкодера, відповідно до рівняння \ref{eqn:enc_features},

$F_{seg}$ та $F_{cls}$ - нейронні мережі декодера сегментації та класифікації, 

$M_{seg}$ та $C_{cls}$ - результати сегментації та класифікації відповідно до рівнянь \ref{eqn:m_seg} та \ref{eqn:c_cls}

$\hat{M}_{seg}$ та $\hat{C}_{cls}$ - нормовані результати сегментації та класифікації відповідно до рівнянь \ref{eqn:m_hat_seg} та \ref{eqn:c_hat_cls}

В основі запропонованого методу лежить ітеративне уточнення карти ознак сегментації за допомогою направлення градієнтів від задачі класифікації. В даному випадку, під час навчання декодер локалізації використовується в ролі механізму уваги \cite{attention}, що навчається автоматично за рахунок градієнтів до задачі класифікації. Хоча, при використанні даного методу, якість семантичної сегментації є низькою в районі контурів об'єктів, отриманих результатів достатньо для локалізації цих об'єктів на зображеннях. 

Запропонований метод складається з двох етапів: етапу навчання та етапу прогнозування. 





\subsection{Напівавтоматичне навчання ШНМ в задачі локалізації}
\label{unsup-method}

Першим етапом, обчислюється уточнена ознак класифікації. Для цього, обчислюється добуток Адамара між нормованим за допомогою сигмоїдної функції виходом декодера сегментації та логітами класифікації:

\begin{equation}
	M_{unsup} = \hat{M}_{seg} \circ C_{cls}
\end{equation}

Далі, для отримання результату класифікації виконується сумація елементів $M_{unsup}$ з нормалізацією за сумою елементів оригінальної ненормалізованої карти сегментації:

\begin{equation}
	C_{unsup} = \frac{\sum_{h=0}^H \sum_{w=0}^{W} M_{unsup(h,w)}}{\sum_{h=0}^H \sum_{w=0}^{W} M_{seg(h,w)} + c}
\end{equation}

Для чисельної стабільності, до знаменника додано малу константу $c \approx 10^{-5}$

Оскільки масштаб нормованого виходу декодера сегментації знаходиться на проміжку $[0, 1]$, використання добутку Адамара дозволяє розглядати $\hat{M}_{seg}$ як карту важливості регіонів для задачі класифікації. В процесі навчання нейронної мережі, така структура поєднання задач спонукає нейронну мережу до призначення високих значень ($\hat(M)_{seg} \rightarrow 1$) для важливих ознак, що є спільними на зображеннях з навчального набору даних.

Для запобігання вивченню нейронною мережею карт сегментації, що складаються виключно з високих значень, необхідна така ініціалізація параметру зсуву останнього шару декодера сегментації $F_{seg}$, щоб активації, за замовчуванням, були близькими до нуля. Для сигмоїдної функції активації таке значення дорівнює $-4.6$


\subsection{Прогнозування результатів сегментації}

На етапі прогнозування використовується як декодер класифікації, так і декодер, що відповідає за задачу локалізації. Тільки у тому випадку, коли вихід декодера класифікації перевищує заданий поріг $T_c$, виконується процедура декодування сегментації.

Оскільки виходи декодера сегментації $M_{unsup}$ є неперервними, а їхній масштаб визначається процесом навчання, поріг бінаризації сегментації $T_{seg}$ може бути різним для різних зображень. Для вибору оптимального порогу $T_{seg}$ на кожному з вхідних зображень, в процесі пост-обробки запропоновано використати адаптивну бінарізацію за методом Оцу \cite{otsu}, щоб уникнути необхідності калібрації прогнозів нейронної мережі.

Для цього, вихід декодера сегментації $M_{unsup}$ квантизується до 256 значень $M_q$, після чого ітеративно знаходиться поріг $T_{seg}$, що мінімізує дисперсію всередині каналів маски для кожного з класів, яка визначається як зважена сума дисперсій класів переднього та заднього плану:

\begin{equation}
\sigma_{w}^{2}(T_{seg})=min: \; \omega_{0}(t)\sigma_{0}^{2}(t)+\omega_{1}(t)\sigma_{1}^{2}(t)
\end{equation}

Тут $\omega_0$, $\omega_1$ - ймовірності класів при розділенні порогом $t$, а $\sigma_0^2$ та $\sigma_1^2$ - дисперсії класів. 

Після виконання бінаризації, над отриманою маскою $M_t = M_q > T_{seg}$ виконується морфологічна операція ерозії \cite{morph_opening} з квадратним ядром розміру 1\% від розміру зображення для того, щоб позбутися малих можливо хибно-позитивних регіонів маски: 

\begin{equation}
M_e = M_t \ominus k
\end{equation}

де $k$ - ядро ерозії.

Результатом такого перетворення э бінаризована карта сегментації, в якій розмічено найбільш вірогідні ознаки, що вплинули на результат класифікації. 

% 2 страницы ------------------------------------------------------------------------
\section{Висновки до третього розділу}


\begin{enumerate}
	\item Запропоновано модель багатозадачної нейронної мережі, що дозволяє вирішувати задачі семантичної сегментації та класифікації для аналізу планарних зображень. 
	
	\item Виявлено, що окрім загальної архітектури моделі, критичними для коректної роботи запропонованих методів є структури декодерів для кожної з задач, зокрема, нормалізація їх внутрішніх представлень.
	
	\item На основі запропонованої моделі багатозадачної нейронної мережі розроблено метод багатозадачного навчання, що в умовах частково-помилкової розмітки дозволяє зменшити вплив неправильно розмічених прикладів на процес навчання за рахунок їх ефективної фільтрації безпосередньо під час навчання. 
	
	\item Запропоновано метод автоматичної фільтрації помилкової розмітки, що спирається на відсіювання градієнтів в точках, що відповідають занадто високим значення функції втрат.
	
	\item Запропоновано використання задачі класифікації як більш загальної до задачі семантичної сегментації при багатозадачному навчанні для оновлення параметрів енкодера в точках, що були помилково відфільтровані як зашумлені.
	
	\item Удосконалено метод об'єднання задач сегментації та класифікації на етапі прогнозування для зменшення кількості хибно-позитивних результатів.

	\item На основі двох представлених методів, розроблено метод навчання з частковим залученням вчителя та метод пост-обробки, що дозволяє виконувати локалізацію важливих для класифікації ознак на вихідному зображенні, що є корисним під час інтерпретації прогнозів моделей.
	
\end{enumerate}

