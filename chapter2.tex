\chapter{Методи поєднання задач глибинного багатозадачного навчання}
Основная идея - использование точной, но более грубой разметки для улучшения внутренних репрезентаций нейросети с последующим улучшением предсказаний с учетом неточной разметки на основе хороших предсказаний













% 10 страниц ------------------------------------------------------------------------
\section{Метод багатозадачного навчання ШНМ в умовах частково помилкової розмітки навчального набору даних}

Для підвищення точності нейронних мереж в умовах навчання з частково-помилковою розміткою навчальних даних запропоновано метод багатозадачного навчання з використанням різних похідних від оригінальної задачі. 


\subsection{Використання семантично-близьких задач під час навчання нейронних мереж}

Использование image-level классов как подсказки для улучшения признаков, которые выучивает нейросеть  во время обучения

Использование truncated loss для более ошибочных данных 


Toy model по сегментации (simple unet) и контроль уровня шума и метрик

% 10 страниц ------------------------------------------------------------------------
\section{Метод зниження кількості хибно-позитивних прогнозів нейронної мережі за рахунок використання результатів семантично-близьких задач}

% 5 страниц
\subsection{Використання задачі класифікації для покращення семантичної сегментації}

Вероятностная модель P(Seg | C) при рассмотрении сегментации как попиксельной классификации при условии, что классификатор уровня картинки выдал свой вердикт

Совмещение классификатора уровня картинки через умножение вероятностей

Обоснование связи с соответствующим обучением

Убирает маленькие и неуверенные регионы


% 5 старниц
\subsection{Використання близьких задач до задачі класифікації}


Внесение uncertainty через преобразование классификации в регрессию, усреднения с регрессией и последующей квантизацией



% 10 страниц ------------------------------------------------------------------------
\section{Метод сегментації важливих для класифікації ознак зображення в умовах відсутності розмітки для сегментації в навчальному наборі даних}

На основі двох представлених методів, запропоновано метод навчання з частковим залученням вчителя та метод пост-обробки, що дозволяє виконувати  сегментацію важливих для класифікації ознак. Даний метод складається з двох етапів: етапу навчання та етапу прогнозування. 

Використані позначення:



$x$ - вхідне зображення енкодера розмірами $3 \times H \times W$ для RGB зображень

$f_{encoder}(x)$ - функція енкодера ознак

$v_e$ - вихідна карта ознак енкодера, розмірами $C \times \frac{H}{r} \times \frac{W}{r}$, де $r$ - константа масштабування

$f_{classifier}$ - функція декодера класифікації

$f_{segmentation}$ - функція декодера сегментації

\subsection{Напівавтоматичне навчання ШНМ в задачі сегментації}

Для навчання декодера сегментації з частковим залученням вчителя, запропоновано 

\begin{align}
v_e &= f_{encoder}(x) \\
C &= f_{classifier}(v_e ) \\
M &= f_{segmentation}(v_e)
\end{align}

\begin{equation}
M_{refined} = \sigma(M) \circ C
\end{equation}

\begin{equation}
	\sigma = \frac{1}{1 + e^{-M}}
\end{equation}

\begin{equation}
C_{refined} = \frac {\sum_{h}^{H} \sum_{w}^{W} M_{refined(hw)}}{\sum_{h}^{H} \sum_{w}^{W} \sigma (M_{hw})}
\end{equation}

\subsection{Прогнозування результатів сегментації}

На етапі прогнозування використовується як декодер класифікації, так і декодер, що відповідає за задачу сегментації. Тільки у тому випадку, коли вихід декодера класифікації перевищує заданий поріг $T_c$, виконується процедура декодування сегментації.

Оскільки виходи декодера сегментації $M = D(X)$ є неперервними, а їхній масштаб визначається процесом навчання, поріг бінаризації сегментації $T_s$ може бути різним для різних зображень. Для вибору оптимального порогу $T_s$ на кожному з вхідних зображень, в процесі пост-обробки запропоновано використати адаптивну бінарізацію за методом Оцу \cite{otsu}, щоб уникнути необхідності калібрації прогнозів нейронної мережі.

Для цього, вихід декодера сегментації $M$ квантизується до 256 значень $M_q$, після чого ітеративно знаходиться поріг $T_s$, що мінімізує дисперсію всередині класів, яка визначається як зважена сума дисперсій класів переднього та заднього плану:

\begin{equation*}
\sigma_{w}^{2}(T_s)=min: \; \omega_{0}(t)\sigma_{0}^{2}(t)+\omega_{1}(t)\sigma_{1}^{2}(t)
\end{equation*}

Тут $\omega_0$, $\omega_1$ - ймовірності класів при розділенні порогом $t$, а $\sigma_0^2$ та $\sigma_1^2$ - дисперсії класів. 

Після виконання бінаризації, над отриманою маскою $M_t = M_q > T_s$ виконується морфологічна операція ерозії \cite{morph_opening} з квадратним ядром розміру 1\% від розміру зображення для того, щоб позбутися малих можливо хибно-позитивних регіонів маски: 

\begin{equation*}
M_e = M_t \ominus k
\end{equation*}
де $k$ - ядро ерозії.

Приклад роботи описаного алгоритму по кроках зображено на рисунку \ref{fig:res_unsup_decoding}.

% фигуру с алгоритмом сюда и картинку с примерами каждого из шагов
\begin{figure}[H]
	\centering
	\includegraphics[width=14cm]{res_unsup_decoding.png}
	\caption{Приклад роботи сегментації на зображенні меланоми}
	\label{fig:res_unsup_decoding}
\end{figure} 

Для наочності, контури маски зображено на оригінальному зображенні (червоним).

% 2 страницы ------------------------------------------------------------------------
\subsection{Висновки до другого розділу}
% Как круо все получилось и пора показать эксперименты
% В экспериментах надо показать сравнение shap и полученного метода для сегментации
