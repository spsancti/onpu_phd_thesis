\chapter{Методи поєднання задач глибинного багатозадачного навчання}
Основная идея - использование точной, но более грубой разметки для улучшения внутренних репрезентаций нейросети с последующим улучшением предсказаний с учетом неточной разметки на основе хороших предсказаний


\section{Модель шумних даних в задачі семантичної сегментації}

Для систематизованого вивчення впливу рівня шуму в даних, та відповідного впливу запропонованих рішень, розроблено модель зашумлених даних задачі сегментації, що відповідає оцінкам моделей шуму в різних задачах автоматизованого скринінгу. 

Основними проблемами з розміткою даних в задачах скринінгу є:
\begin{itemize}
	\item Маски сегментації, що захоплюють сусідні з об'єктом пікселі \cite{severstal}
	\item Маски сегментації, що покривають об'єкт не повністю \cite{clouds}
	\item Відсутні маски сегментації для деяких об'єктів \cite{severstal}
	\item Присутні зайві маски на місцях, де немає об'єктів \cite{clouds}
\end{itemize}

Для моделювання цих дефектів розмітки запропоновано метод контрольованої генерації як зображень, так і масок сегментації що модифікуються відповідно до випадково обраних недоліків.

Контрольована генерація зображень виконується в два етапи: генерація фону та розташування на ньому довільної кількості об'єктів. Для генерації фону можуть бути використані як звичайні натуральні зображення, так і синтетичні текстури, чи заливка константним кольором. 

В якості об'єктів використовуються зображення з набору даних MNIST \cite{mnist}. Використання набору даних MNIST зумовлену трьом факторами: 
\begin{itemize}
	\item Можливість простого відділення об'єкта від фону;
	\item Наявність схожих елементів в різних класів (наприклад, цифри 1 та 7);
	\item Висока точність роботи сучасних нейронних мереж, що дозволяє сконцентруватися на впливі шуму в розмітці
\end{itemize}

Помилки в розмітку штучно вводяться за допомогою випадкового застосування морфологічних операцій ерозії та дилатації з квадратним ядром до масок сегментації окремих об'єктів перед додаванням їх до загальної маски. 

Окрім наборів даних зображень фону $\mathcal{X}_{b}$ та об'єктів $\mathcal{X}_{f}$, модель має наступні параметри:

\begin{itemize}
	\item Розмір зображення $S_{i}$ в пікселях
	\item Середній розмір об'єкта $S_{o}$ в пікселях
	\item Дисперсія розміру об'єкта $\sigma_{o}$
	\item Максимальна кількість об'єктів на зображенні $N$
	\item Ймовірності ерозії та дилатації маски кожного з об'єктів $P_e$ та $P_d$
	\item Розміри ядер ерозії та дилатації масок всіх об'єктів $S_e$ та $S_d$	
\end{itemize}

На основі моделі генеруються навчальний та тестовий набори даних, всі зображення в наборах генеруються незалежно.

Алгоритм генерації кожного з зображень складається з наступних кроків:

1. Вибір випадкового зображення фону: $x_b \sim \mathcal{X}_b$

2. Вибір кількості об'єктів на зображенні: $n_o \sim \mathcal{U}(1, N)$

3. Ініціалізація маски сегментації: $M = 0_{S_i,S_i}$

4. Відповідно до кількості зображень $n_o$ виконати наступні кроки:

4.1 Вибрати зображення об'єкта $x_f \sim \mathcal{X}_f$

4.2 Вибрати розміри об'єкта: $s \sim \mathcal{U}(S_o - \sigma_o, S_o + \sigma_o)$

4.3 Змінити розмір зображення об'єкта за допомогою білінійної інтерполяції: 
\begin{align*}
	\hat{x}_f = R_{bilinear}(x_f)
\end{align*}

4.4 Вибрати координати розміщення об'єкта: 
\begin{align*}
	i_f &\sim \mathcal{U}(0, S_i - s)\\
	j_f &\sim \mathcal{U}(0, S_i - s)
\end{align*}

4.4 Розмістити зображення об'єкта на зображенні фону:
\begin{equation*}
x_b[i_f...i_f+s, j_f...j_f+s] = max(x_b[i_f...i_f+s, j_f...j_f+s], x_f)
\end{equation*}

% 10 страниц ------------------------------------------------------------------------
\section{Метод багатозадачного навчання ШНМ в умовах частково помилкової розмітки навчального набору даних}

Для підвищення точності нейронних мереж в умовах навчання з частково-помилковою розміткою навчальних даних запропоновано метод багатозадачного навчання з використанням задач, похідних від оригінальної. 

Запропонований метод спирається на припущення, що існує семантично-близька більш загальна задача, для якої розмітка навчальних даних є більш влучною, ніж для вихідної задачі. 

На відміну від \cite{arxiv:1412.0069}, що спирається на вивчення більш детальних семантично-близьких задач, в розробленому методі використання більш точних даних, для загальніших задач дозволяє покращити роздільність внутрішніх представлень нейронної мережі, що, в свою чергу, покращує результати на вихідній задачі.

Також, оскільки задачі є семантично близькими, не відбувається конфлікту градієнтів, що є типовим при навчанні семантично-різнорідних задач \cite{gradient_conflict}.

\subsection{Задача семантичної сегментації}

Більш загальною до задачі семантичної сегментації є задача класифікації. В такому контексті, задача класифікації зводиться до задачі навчання за набором зразків \cite{multiinstance}: замість розмітки кожного з об'єктів для всіх класів на зображенні, зображення являє собою мішок з одним, чи декількома об'єктами та відповідним маркуванням, чи є об'єкти заданих класів на зображенні.

Зазвичай, неточна розмітка в задачах сегментації полягає в наявності зайвих, або у відсутності деяких розмічених пікселів. Наявність такої неточної розмітки дозволяє створити на її основі точну розмітку для класифікації: якщо на зображенні $x \in \mathcal{X}$ є хоча б один розмічений об'єкт класу $C$, встановлюється мітка відповідного класу $y_c \in (0, 1)$ в задачі класифікації.



%2 страницы
%Использование image-level классов как подсказки для улучшения признаков, которые выучивает нейросеть  во время обучения
%	Помощь в разделении признаков на самом глубоком уровне сети
%	Возможность использования semi-supervised для неразмеченной сегментации
%	Важность average+max пулинга
	
% 2 страницы, есть 1 страница
\paragraph{Обмеження зверху функції втрат для задачі з менш точною розміткою}

Такі функції втрат, як перехресна ентропія, або фокальна функція втрат призначають експоненційно високе значення для неправильних прогнозів нейронних мереж. Це має шкідливий ефект при наявності помилкової розмітки в тренувальному наборі даних - правильні прогнози на неправильно-розмічених даних створюють конфліктні градієнти та знижують впевненість прогнозів нейронної мережі.

Для зменшення впливу неправильної розмітки сегментації на навчання запропоновано обмеження функції втрат зверху, таким чином,при навчанні на декількох задачах, градієнти від неправильної розмітки будуть мати менший вплив на процес навчання:

\begin{equation*}
	\mathcal{L} \rceil = min(L, \theta)
\end{equation*}

де $\theta$ - поріг обмеження функції втрат.

Графіки обмежених зверху функцій втрат зображено на рисунку \ref{fig:trimmed_losses}.
\ref{fig:trimmed_losses}
\begin{figure}
	\centering
	\includegraphics[width=12cm]{trimmed_losses_en.png}
	\caption{Приклади обмежених зверху функцій втрат}
	\label{fig:trimmed_losses}
\end{figure} 

Для функції обмеження зверху градієнт визначено лише на проміжку $(-\inf, \theta]$, тому для проміжку $(\theta, \inf)$ градієнт встановлено рівним нулю:

\begin{equation*}
\mathcal{\nabla} min(L, \theta) = 
\begin{cases}
	1 &\text{$L \in (-\inf, \theta]$}\\
	0 &\text{$L \in (\theta, \inf)$}
\end{cases}
\end{equation*}

Таким чином, приклади з занадто великою похибкою (переважно з помилковою розміткою) ефективно виключаються з навчання. Також, випадково можуть бути виключені з навчання приклади, що занадто складні для вивчення нейронною мережею на ранніх етапах. В такому випадку, навчання на додатковій, більш загальній задачі та більш простих прикладах дозволяє підготувати нейронну мережу до вивчення складніших випадків. 



\paragraph{Експериментальні дослідження на шумних даних}
% 2 страницы
Экперименты на toy model
	Сегментация онли - tsne фичей + метрики
		Без шума
		С шумом 1
		С шумом 2
	
	Сегментация + классификация - tsne фичей + метрики
		Без шума
		С шумом 1
		С шумом 2


% 10 страниц ------------------------------------------------------------------------
\section{Метод зниження кількості хибно-позитивних прогнозів нейронної мережі за рахунок використання результатів семантично-близьких задач}

% 5 страниц
\subsection{Використання задачі класифікації для покращення семантичної сегментації}

% 2 страницы
Вероятностная модель P(S | C) при рассмотрении сегментации как попиксельной классификации при условии, что классификатор уровня картинки выдал свой вердикт

% 1 страница
Совмещение классификатора уровня картинки через умножение вероятностей

% 1 страница
Обоснование связи с соответствующим обучением

% 1 страинца
Обоснование уменьшения false positives


% 5 старниц
\subsection{Використання близьких задач до задачі класифікації}

% 2 страницы
Внесение uncertainty через преобразование классификации в регрессию, усреднения с регрессией и последующей квантизацией


% 3 страницы
Регуляризация преобразования в регрессию





% 10 страниц ------------------------------------------------------------------------
\section{Метод сегментації важливих для класифікації ознак зображення в умовах відсутності розмітки для сегментації в навчальному наборі даних}

На основі двох представлених методів, запропоновано метод навчання з частковим залученням вчителя та метод пост-обробки, що дозволяє виконувати  сегментацію важливих для класифікації ознак. Даний метод складається з двох етапів: етапу навчання та етапу прогнозування. 

Використані позначення:



$x$ - вхідне зображення енкодера розмірами $3 \times H \times W$ для RGB зображень

$f_{encoder}(x)$ - функція енкодера ознак

$v_e$ - вихідна карта ознак енкодера, розмірами $C \times \frac{H}{r} \times \frac{W}{r}$, де $r$ - константа масштабування

$f_{classifier}$ - функція декодера класифікації

$f_{segmentation}$ - функція декодера сегментації

\subsection{Напівавтоматичне навчання ШНМ в задачі сегментації}

Для навчання декодера сегментації з частковим залученням вчителя, запропоновано 

\begin{align}
v_e &= f_{encoder}(x) \\
C &= f_{classifier}(v_e ) \\
M &= f_{segmentation}(v_e)
\end{align}

\begin{equation}
M_{refined} = \sigma(M) \circ C
\end{equation}

\begin{equation}
	\sigma = \frac{1}{1 + e^{-M}}
\end{equation}

\begin{equation}
C_{refined} = \frac {\sum_{h}^{H} \sum_{w}^{W} M_{refined(hw)}}{\sum_{h}^{H} \sum_{w}^{W} \sigma (M_{hw})}
\end{equation}

\subsection{Прогнозування результатів сегментації}

На етапі прогнозування використовується як декодер класифікації, так і декодер, що відповідає за задачу сегментації. Тільки у тому випадку, коли вихід декодера класифікації перевищує заданий поріг $T_c$, виконується процедура декодування сегментації.

Оскільки виходи декодера сегментації $M = D(X)$ є неперервними, а їхній масштаб визначається процесом навчання, поріг бінаризації сегментації $T_s$ може бути різним для різних зображень. Для вибору оптимального порогу $T_s$ на кожному з вхідних зображень, в процесі пост-обробки запропоновано використати адаптивну бінарізацію за методом Оцу \cite{otsu}, щоб уникнути необхідності калібрації прогнозів нейронної мережі.

Для цього, вихід декодера сегментації $M$ квантизується до 256 значень $M_q$, після чого ітеративно знаходиться поріг $T_s$, що мінімізує дисперсію всередині класів, яка визначається як зважена сума дисперсій класів переднього та заднього плану:

\begin{equation*}
\sigma_{w}^{2}(T_s)=min: \; \omega_{0}(t)\sigma_{0}^{2}(t)+\omega_{1}(t)\sigma_{1}^{2}(t)
\end{equation*}

Тут $\omega_0$, $\omega_1$ - ймовірності класів при розділенні порогом $t$, а $\sigma_0^2$ та $\sigma_1^2$ - дисперсії класів. 

Після виконання бінаризації, над отриманою маскою $M_t = M_q > T_s$ виконується морфологічна операція ерозії \cite{morph_opening} з квадратним ядром розміру 1\% від розміру зображення для того, щоб позбутися малих можливо хибно-позитивних регіонів маски: 

\begin{equation*}
M_e = M_t \ominus k
\end{equation*}
де $k$ - ядро ерозії.

Приклад роботи описаного алгоритму по кроках зображено на рисунку \ref{fig:res_unsup_decoding}.

% фигуру с алгоритмом сюда и картинку с примерами каждого из шагов
\begin{figure}[H]
	\centering
	\includegraphics[width=14cm]{res_unsup_decoding.png}
	\caption{Приклад роботи сегментації на зображенні меланоми}
	\label{fig:res_unsup_decoding}
\end{figure} 

Для наочності, контури маски зображено на оригінальному зображенні (червоним).

% 2 страницы ------------------------------------------------------------------------
\subsection{Висновки до другого розділу}
% Как круо все получилось и пора показать эксперименты
% В экспериментах надо показать сравнение shap и полученного метода для сегментации
